# -*- coding: utf-8 -*-
"""BP_train_models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mTZwZ1V9COjAfIkyleXfnLvNG4jjII5R
"""

#제작한 세 종류의 데이터셋으로 모델을 훈련하는 코드. 훈련 결과물은 Pretrained_Model, Pretrained_Model_sentence_embed, Finetuned_Model_judge_question, Finetuned_Model_judge_answer 4종류

!pip install soynlp
!pip install datasets
!pip install accelerate -U
!pip install kss
!pip install evaluate
import evaluate
import accelerate
import random
import pandas as pd
import numpy as np
import re
import os
import torch
import tensorflow as tf
import urllib.request
import torch.nn as nn
from kss import Kss
from transformers.modeling_outputs import SequenceClassifierOutput
from datasets import Dataset, load_dataset, ClassLabel, load_metric
from soynlp.word import WordExtractor
from soynlp.tokenizer import LTokenizer, MaxScoreTokenizer
from tqdm import tqdm
from torch.nn.functional import cross_entropy
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.preprocessing.sequence import pad_sequences
from transformers import AutoModel, AutoTokenizer, shape_list, TFBertModel, RobertaTokenizerFast, RobertaForSequenceClassification, TextClassificationPipeline, pipeline, BertTokenizer, BertForNextSentencePrediction,  TrainingArguments, BertForMaskedLM, Trainer, TrainerCallback, BertConfig, BertForSequenceClassification
from sklearn.model_selection import StratifiedKFold, train_test_split
from evaluate import evaluator
from google.colab import drive
drive.mount('/content/drive')

from transformers.models.bert.modeling_bert import BertModel, BertPooler

#과적합 방지를 위한 EarlyStopping 클래스
class EarlyStoppingCallback(TrainerCallback):
    def __init__(self, patience=3):
        self.patience = patience
        self.counter = 0
        self.best_score = None
        self.early_stop = False

    def on_evaluate(self, args, state, control, **kwargs):
        # 현재 검증 손실 가져오기
        current_loss = kwargs['metrics']['eval_loss']

        # 최고 성능 갱신 또는 카운터 증가
        if self.best_score is None:
            self.best_score = current_loss
        elif current_loss > self.best_score:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
                control.should_training_stop = True
        else:
            self.best_score = current_loss
            self.counter = 0

#Bert 모델의 pooler 층에서 activation 함수를 gelu로 변경
class CustomBertPooler(BertPooler):
    def forward(self, hidden_states):
        first_token_tensor = hidden_states[:, 0]
        pooled_output = torch.nn.functional.gelu(self.dense(first_token_tensor))  # Changed from tanh to gelu
        return pooled_output

class CustomBertForSequenceClassification(BertForSequenceClassification):
    def __init__(self, config):
        super().__init__(config)
        self.bert.pooler = CustomBertPooler(config)  # Replace the pooler

    @classmethod
    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):
        model = super().from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)
        model.bert.pooler = CustomBertPooler(model.config)  # Ensure pooler is replaced in loaded model
        return model

#Text classification에 사용할 metric 함수
#한 번에 Precision, Recall, F1 score 측정할 것이기 때문에 Trainer() 사용하기

# Metric 로드
accuracy_metric = evaluate.load("accuracy")
precision_metric = evaluate.load("precision")
recall_metric = evaluate.load("recall")
f1_metric = evaluate.load("f1")

# compute_metrics 함수 정의
def compute_metrics(p):
    predictions, labels = p
    predictions = predictions.argmax(axis=1)

    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)
    precision = precision_metric.compute(predictions=predictions, references=labels, average="weighted")
    recall = recall_metric.compute(predictions=predictions, references=labels, average="weighted")
    f1 = f1_metric.compute(predictions=predictions, references=labels, average="weighted")

    return {
        "accuracy": accuracy["accuracy"],
        "precision": precision["precision"],
        "recall": recall["recall"],
        "f1": f1["f1"]
    }

#Summarization에 사용할 metric

#Feature Extraction에 사용할 metric

"""#MLM으로 비지도 학습(KcBERT, KoSimCSE_BERT)"""

#훈련 데이터 불러오기
pretrain_dataset_path = '/content/drive/My Drive/KcBERT_pretrain_dataset_short.csv'
df_pretrain = pd.read_csv(pretrain_dataset_path, index_col = 0)
print(df_pretrain)

#soynlp tokenizer를 Pretrain 데이터로 학습
pretrain_list = df_pretrain['text'].to_list()

word_extractor = WordExtractor(min_frequency = 5)
word_extractor.train(pretrain_list)
word_score_table = word_extractor.extract()

print(word_score_table['picker'])

# 예제로 몇 가지 단어 점수 확인
for word, score in word_score_table.items():
    print(f"{word}: {score.cohesion_forward:.2f}")

cohesion_score = {word:score.cohesion_forward for word, score in word_score_table.items()}

Ltokenizer = LTokenizer(scores = cohesion_score)
Maxtokenizer = MaxScoreTokenizer(scores = cohesion_score)

sentence = '자바스크립트 개발 경험이 있으신 경우라면 DateTime Picker 엘레먼트에 ID를 부여해서 자바스크립트 코드로 date 함수와 DOM을 핸들링해서 직접 처리해서 쉽게 구현하실 수가 있을 듯 합니다.'

print(Ltokenizer.tokenize(sentence))
print(Maxtokenizer.tokenize(sentence))

sentence = ' 지원하는 기능이 많을수록 느립니다. BDK는 확실히 로딩속도가 느립니다.Natively는 기능이 적은 대신에 로딩속도는 조금 빠른 편이더군요. :)'

print(Ltokenizer.tokenize(sentence))
print(Maxtokenizer.tokenize(sentence))

# 높은 cohesion 점수를 가진 단어들을 선택하여 단어사전 구성
threshold = 0.1  # 점수 기준 설정
new_vocab = [word for word, score in word_score_table.items() if score.cohesion_forward > threshold]

# 선택된 단어 출력
print(new_vocab)

#MLM을 위한 데이터셋의 일부를 [MASK] 토큰으로 교체하는 함수
def mask_input_ids(input_ids, tokenizer, mlm_probability=0.15):
    labels = input_ids.clone()

    #각 입력에서 특수 토큰은 마스킹하지 않도록 설정
    special_tokens_mask = [
      torch.tensor(tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True), dtype = torch.bool)
      for val in input_ids]
    special_tokens_mask = torch.stack(special_tokens_mask)

    #마스킹 확률을 적용해 마스크 생성
    probability_matrix = torch.full(labels.shape, mlm_probability)
    probability_matrix.masked_fill_(special_tokens_mask, value=0.0)
    masked_indices = torch.bernoulli(probability_matrix).bool()

    # PyTorch에서 사용하는 masked loss 계산을 위해 마스킹 되지 않은 부분은 -100으로 설정
    labels[~masked_indices] = -100

    # `[MASK]` 토큰으로 대체
    input_ids[masked_indices] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)
    return input_ids, labels

"""#KcBERT 훈련"""

#Huggingface의 원본 모델 로드
#BertForMaskedLM 구조에 맞게 모델 불러오기
HUGGINGFACE_MODEL_PATH = 'beomi/kcbert-base'

KcBERT_tokenizer = AutoTokenizer.from_pretrained(HUGGINGFACE_MODEL_PATH)
KcBERT_model = BertForMaskedLM.from_pretrained(HUGGINGFACE_MODEL_PATH)

# Trainer가 알아서 gpu를 감지하고 모델과 데이터를 gpu로 옮겨줌
#GPU 사용 코드
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#파이프라인을 사용하는 경우 GPU로 모델을 옮겨준 뒤 파이프라인에 모델을 넣어줘야 함
KcBERT_model.to(device)

# 새로운 단어 토크나이저에 추가
num_added_toks = KcBERT_tokenizer.add_tokens(new_vocab)
print(f"Added {num_added_toks} tokens")

# 모델의 임베딩 크기 조정
KcBERT_model.resize_token_embeddings(len(KcBERT_tokenizer))

#MLM 데이터셋 준비
corpus = pretrain_list
tokenized_corpus = KcBERT_tokenizer(corpus, add_special_tokens = True, max_length = KcBERT_model.config.max_position_embeddings, truncation = True, padding = 'max_length', return_tensors = 'pt')

print(tokenized_corpus)

#MLM을 위한 masked 데이터셋 생성(MLM에서는 stratify 적용하지 않음.)
masked_inputs, labels = mask_input_ids(tokenized_corpus['input_ids'], KcBERT_tokenizer)
pretrain_dict = {'input_ids' : masked_inputs, 'attention_mask' : tokenized_corpus['attention_mask'], 'labels' : labels}
pretrain_dataset = Dataset.from_dict(pretrain_dict)
pretrain_dataset = pretrain_dataset.train_test_split(test_size = 0.1)

# 훈련 설정
training_args = TrainingArguments(
    output_dir = './results',
    evaluation_strategy = 'steps',
    eval_steps = 500,
    save_strategy = "steps",
    save_steps = 500,
    num_train_epochs = 3,
    save_total_limit = 3,
    per_device_eval_batch_size = 8,
    per_device_train_batch_size = 8,
    warmup_steps = 300, #고려사항
    weight_decay = 0.01, #고려사항
    logging_dir = "./logs",
    load_best_model_at_end = True
)

# 트레이너 생성
trainer = Trainer(
    model = KcBERT_model,
    args = training_args,
    train_dataset = pretrain_dataset['train'],
    eval_dataset = pretrain_dataset['test'],
    callbacks = [EarlyStoppingCallback(patience = 5)]
)

# 훈련 시작
trainer.train()

#google drive에 학습된 모델을 저장
save_path = '/content/drive/My Drive/Pretrained_Model'

KcBERT_model.save_pretrained(save_path)
KcBERT_tokenizer.save_pretrained(save_path)

"""#KoSimCSE_BERT 훈련"""

#먼저 pooler 층을 변수로 저장해두기 위해 모델을 AutoModel로 로드
HUGGINGFACE_MODEL_PATH = 'BM-K/KoSimCSE-bert-multitask'

KoSim_model = AutoModel.from_pretrained(HUGGINGFACE_MODEL_PATH)

#pooler 층의 파라미터만 변수로 저장
pooler_layer = KoSim_model.pooler

#MLM으로 domain adaptation을 진행하기 위해 모델을 BertForMaskedLM으로 로드
HUGGINGFACE_MODEL_PATH = 'BM-K/KoSimCSE-bert-multitask'

KoSim_model = BertForMaskedLM.from_pretrained(HUGGINGFACE_MODEL_PATH)
KoSim_tokenizer = AutoTokenizer.from_pretrained(HUGGINGFACE_MODEL_PATH)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
KoSim_model.to(device)

print(KoSim_model.pooler.dense.weight)

# 새로운 단어 토크나이저에 추가
num_added_toks = KoSim_tokenizer.add_tokens(new_vocab)
print(f"Added {num_added_toks} tokens")

# 모델의 임베딩 크기 조정
KoSim_model.resize_token_embeddings(len(KoSim_tokenizer))

#MLM 데이터셋 준비
corpus = pretrain_list
tokenized_corpus = KoSim_tokenizer(corpus, add_special_tokens = True, max_length = KoSim_model.config.max_position_embeddings, truncation = True, padding = 'max_length', return_tensors = 'pt')

print(tokenized_corpus)

#MLM을 위한 masked 데이터셋 생성(MLM에서는 stratify 적용하지 않음.)
masked_inputs, labels = mask_input_ids(tokenized_corpus['input_ids'], KoSim_tokenizer)
pretrain_dict = {'input_ids' : masked_inputs, 'attention_mask' : tokenized_corpus['attention_mask'], 'labels' : labels}
pretrain_dataset = Dataset.from_dict(pretrain_dict)
pretrain_dataset = pretrain_dataset.train_test_split(test_size = 0.1)

# 훈련 설정
training_args = TrainingArguments(
    output_dir = './results',
    evaluation_strategy = 'steps',
    eval_steps = 500,
    save_strategy = "steps",
    save_steps = 500,
    num_train_epochs = 3,
    save_total_limit = 3,
    per_device_eval_batch_size = 8,
    per_device_train_batch_size = 8,
    warmup_steps = 300, #고려사항
    weight_decay = 0.01, #고려사항
    logging_dir = "./logs",
    load_best_model_at_end = True
)

# 트레이너 생성
trainer = Trainer(
    model = KoSim_model,
    args = training_args,
    train_dataset = pretrain_dataset['train'],
    eval_dataset = pretrain_dataset['test'],
    callbacks = [EarlyStoppingCallback(patience = 5)]
)

# 훈련 시작
trainer.train()

#학습이 끝난 후 MLM을 위한 cls층을 제거하고, 변수로 저장해둔 pooler 층을 다시 추가
del KoSim_model.cls
KoSim_model.bert.pooler = pooler_layer
print(KoSim_model)

#google drive에 학습된 모델을 저장
save_path = '/content/drive/My Drive/Pretrained_Model_sentence_embed'

KoSim_model.save_pretrained(save_path)
KoSim_tokenizer.save_pretrained(save_path)

"""#질문 데이터셋으로 파인튜닝"""

#훈련 데이터 불러오기
finetune_dataset_path = '/content/drive/My Drive/KcBERT_finetune_dataset_question.xlsx'
df_finetune = pd.read_excel(finetune_dataset_path, index_col = 0)

#모델 불러오기
pretrained_model_path = '/content/drive/My Drive/Pretrained_Model'

pretrained_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_path)
pretrained_model = BertForSequenceClassification.from_pretrained(pretrained_model_path)

#모델의 pooler층에서 activation function을 tanh에서 gelu로 변경하기

# Trainer가 알아서 gpu를 감지하고 모델과 데이터를 gpu로 옮겨줌
#GPU 사용 코드
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#파이프라인을 사용하는 경우 GPU로 모델을 옮겨준 뒤 파이프라인에 모델을 넣어줘야 함
pretrained_model.to(device)

#훈련 데이터셋의 결측치 제거
print(df_finetune[pd.isna(df_finetune['text']) == True])
df_finetune = df_finetune[pd.isna(df_finetune['text']) == False]
print(df_finetune)

#데이터셋 전처리 함수
def tokenize_function(examples):
    return pretrained_tokenizer(examples['text'], truncation = True, padding = "max_length", max_length = pretrained_model.config.max_position_embeddings)

#공통 테스트 데이터셋 제작
#데이터셋 전처리, 훈련/검증 9:1 분리
dataset = Dataset.from_pandas(df_finetune)
finetune_dataset = dataset.map(tokenize_function, batched=True)
finetune_dataset = finetune_dataset.class_encode_column('label')
finetune_dataset = finetune_dataset.train_test_split(test_size = 0.1, stratify_by_column = 'label')

question_train_dataset, question_test_dataset = finetune_dataset['train'], finetune_dataset['test']

"""## 질문 데이터셋에 StratifiedKFold 적용 X"""

# #데이터셋 전처리, 훈련/검증 8:2 분리
# dataset = Dataset.from_pandas(df_finetune)
# finetune_dataset = dataset.map(tokenize_function, batched=True)
# finetune_dataset = finetune_dataset.class_encode_column('label')
# finetune_dataset = finetune_dataset.train_test_split(test_size = 0.2, stratify_by_column = 'label')

# #성능 측정을 위한 데이터 추가 분리

# #학습에 사용할 훈련 데이터, (검증 + 테스트) 데이터 분리
# train_dataset, valid_test_dataset = finetune_dataset['train'], finetune_dataset['test']

# #(검증 + 테스트) 데이터를 1:1로 분리
# valid_test_dataset = valid_test_dataset.train_test_split(test_size = 0.5, stratify_by_column = 'label')
# valid_dataset, question_test_dataset = valid_test_dataset['train'], valid_test_dataset['test']

#성능 측정을 위한 데이터 추가 분리

#학습에 사용할 훈련/검증 데이터 9:1로 분리
question_train_dataset_nofold = question_train_dataset.train_test_split(test_size = 0.1, stratify_by_column = 'label')
train_dataset, valid_dataset = question_train_dataset_nofold['train'], question_train_dataset_nofold['test']

#훈련 설정
training_args = TrainingArguments(
    output_dir = './results',
    learning_rate = 5e-5,
    evaluation_strategy = 'steps',
    eval_steps = 100,
    save_strategy = "steps",
    save_steps = 100,
    num_train_epochs = 3,
    save_total_limit = 3,
    per_device_eval_batch_size = 8,
    per_device_train_batch_size = 8,
    warmup_steps = 100, #고려사항
    weight_decay = 0.01, #고려사항
    logging_dir = "./logs",
    load_best_model_at_end = True
)

# 트레이너 생성
trainer = Trainer(
    model = pretrained_model,
    args = training_args,
    train_dataset = train_dataset,
    eval_dataset = valid_dataset,
    callbacks = [EarlyStoppingCallback(patience = 3)]
)

trainer.train()

#모델 성능 테스트
trainer = Trainer(
    model = pretrained_model,
    compute_metrics = compute_metrics,
)

trainer.predict(question_test_dataset)

#google drive에 학습된 모델을 저장
save_path = '/content/drive/My Drive/Finetuned_Model_judge_question_nofold'

pretrained_model.save_pretrained(save_path)
pretrained_tokenizer.save_pretrained(save_path)

"""## 질문 데이터셋에 StratifiedKFold 적용"""

# #데이터셋 전처리, 훈련/검증 9:1 분리
# dataset = Dataset.from_pandas(df_finetune)
# finetune_dataset = dataset.map(tokenize_function, batched=True)
# finetune_dataset = finetune_dataset.class_encode_column('label')
# finetune_dataset = finetune_dataset.train_test_split(test_size = 0.1, stratify_by_column = 'label')

# question_train_dataset, question_test_dataset = finetune_dataset['train'], finetune_dataset['test']

#분류 문제이며 레이블 간 데이터 수 비율을 고려하기 위해 StratifiedKFold 사용
n_splits = 5

# StratifiedKFold 설정
skf = StratifiedKFold(n_splits=n_splits, shuffle=True)
for fold, (train_idx, val_idx) in enumerate(skf.split(question_train_dataset, question_train_dataset['label'])):

    # 훈련 세트와 검증 세트 분리
    question_train_dataset_fold = question_train_dataset.select(train_idx)
    question_val_dataset_fold = question_train_dataset.select(val_idx)

    # 훈련 설정
    training_args = TrainingArguments(
        output_dir="./results",
        evaluation_strategy = "steps",
        eval_steps = 100,
        save_strategy = "steps",
        save_steps = 100,
        learning_rate = 5e-5,
        num_train_epochs = 3, #고려사항
        per_device_train_batch_size = 8,
        per_device_eval_batch_size = 8,
        weight_decay = 0.01, #고려사항
        logging_dir = "./logs",
        load_best_model_at_end = True

    )

    # 트레이너 초기화 및 훈련
    trainer = Trainer(
        model = pretrained_model,
        args = training_args,
        train_dataset = question_train_dataset_fold,
        eval_dataset = question_val_dataset_fold,
        callbacks = [EarlyStoppingCallback(patience = 3)]
    )

    trainer.train()

#모델 성능 테스트
trainer = Trainer(
    model = pretrained_model,
    compute_metrics = compute_metrics,
)

trainer.predict(question_test_dataset)

#google drive에 학습된 모델을 저장
save_path = f'/content/drive/My Drive/Finetuned_Model_judge_question_{n_splits}fold'

pretrained_model.save_pretrained(save_path)
pretrained_tokenizer.save_pretrained(save_path)

"""#Labeled NSP 데이터셋으로 파인튜닝"""

#훈련 데이터 불러오기
finetune_dataset_path = '/content/drive/My Drive/KcBERT_finetune_dataset_answer.xlsx'
df_finetune = pd.read_excel(finetune_dataset_path, index_col = 0)

#MLM으로 훈련된 모델 로드
#BertForNextSentencePrediction 구조에 맞게 모델 불러오기
pretrained_model_load_path = '/content/drive/My Drive/Pretrained_Model'

pretrained_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_load_path)
pretrained_model = BertForNextSentencePrediction.from_pretrained(pretrained_model_load_path)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
pretrained_model.to(device)

print(df_finetune)
dataset = Dataset.from_pandas(df_finetune)
print(dataset)
print(df_finetune[df_finetune['sent2'].isnull()])

# 데이터셋 전처리 함수
def tokenize_function(examples):
    return pretrained_tokenizer(examples['sent1'], examples['sent2'], truncation = True, padding = "max_length", max_length = pretrained_model.config.max_position_embeddings)

#공통 테스트 데이터셋 제작
#데이터셋 전처리, 훈련/검증 9:1 분리
dataset = Dataset.from_pandas(df_finetune)
finetune_dataset = dataset.map(tokenize_function, batched=True)
finetune_dataset = finetune_dataset.class_encode_column('label')
finetune_dataset = finetune_dataset.train_test_split(test_size = 0.1, stratify_by_column = 'label', seed = 1)

answer_train_dataset, answer_test_dataset = finetune_dataset['train'], finetune_dataset['test']

"""## 답변 데이터셋에 StratifiedKFold 적용 X"""

# # NSP 데이터셋 준비(NSP 태스크에서는 stratify 적용)
# dataset = Dataset.from_pandas(df_finetune)
# finetune_dataset = dataset.map(tokenize_function, batched=True)
# finetune_dataset = finetune_dataset.class_encode_column('label')
# finetune_dataset = finetune_dataset.train_test_split(test_size = 0.2, stratify_by_column = 'label')

# #성능 측정을 위한 데이터 추가 분리

# #학습에 사용할 훈련 데이터, (검증 + 테스트) 데이터 분리
# train_dataset, valid_test_dataset = finetune_dataset['train'], finetune_dataset['test']

# #(검증 + 테스트) 데이터를 1:1로 분리
# valid_test_dataset = valid_test_dataset.train_test_split(test_size = 0.5, stratify_by_column = 'label')
# valid_dataset, answer_test_dataset = valid_test_dataset['train'], valid_test_dataset['test']

#성능 측정을 위한 데이터 추가 분리

#학습에 사용할 훈련/검증 데이터 9:1로 분리
answer_train_dataset_nofold = answer_train_dataset.train_test_split(test_size = 0.1, stratify_by_column = 'label')
train_dataset, valid_dataset = answer_train_dataset_nofold['train'], answer_train_dataset_nofold['test']

#훈련 설정
training_args = TrainingArguments(
    output_dir = './results',
    learning_rate = 2e-5, #pretrain에 비해 낮은 값
    evaluation_strategy = 'steps',
    eval_steps = 200,
    save_strategy = "steps",
    save_steps = 200,
    num_train_epochs = 3,
    save_total_limit = 3,
    per_device_eval_batch_size = 8,
    per_device_train_batch_size = 8,
    warmup_steps = 200, #고려사항
    weight_decay = 0.01, #고려사항
    logging_dir = "./logs",
    load_best_model_at_end = True
)

# 트레이너 생성
trainer = Trainer(
    model = pretrained_model,
    args = training_args,
    train_dataset = train_dataset,
    eval_dataset = valid_dataset,
    callbacks = [EarlyStoppingCallback(patience = 3)]
)

trainer.train()

#모델 성능 테스트
trainer = Trainer(
    model = pretrained_model,
    compute_metrics = compute_metrics,
)

trainer.predict(answer_test_dataset)

#google drive에 학습된 모델을 저장
save_path = '/content/drive/My Drive/Finetuned_Model_judge_answer_nofold'

pretrained_model.save_pretrained(save_path)
pretrained_tokenizer.save_pretrained(save_path)

"""## 답변 데이터셋에 StratifiedKFold 적용"""

# # NSP 데이터셋 준비(NSP 태스크에서는 stratify 적용)
# dataset = Dataset.from_pandas(df_finetune)
# finetune_dataset = dataset.map(tokenize_function, batched=True)
# finetune_dataset = finetune_dataset.class_encode_column('label')
# finetune_dataset = finetune_dataset.train_test_split(test_size = 0.1, stratify_by_column = 'label')

# answer_train_dataset, answer_test_dataset = finetune_dataset['train'], finetune_dataset['test']

#분류 문제이며 레이블 간 데이터 수 비율을 고려하기 위해 StratifiedKFold 사용
n_splits = 5

# StratifiedKFold 설정
skf = StratifiedKFold(n_splits=n_splits, shuffle=True)
for fold, (train_idx, val_idx) in enumerate(skf.split(answer_train_dataset, answer_train_dataset['label'])):

    # 훈련 세트와 검증 세트 분리
    answer_train_dataset_fold = answer_train_dataset.select(train_idx)
    answer_val_dataset_fold = answer_train_dataset.select(val_idx)
    # 훈련 설정
    training_args = TrainingArguments(
        output_dir="./results",
        evaluation_strategy = "steps",
        eval_steps = 100,
        save_strategy = "steps",
        save_steps = 100,
        learning_rate = 5e-5,
        num_train_epochs = 3, #고려사항
        per_device_train_batch_size = 8,
        per_device_eval_batch_size = 8,
        weight_decay = 0.01, #고려사항
        logging_dir = "./logs",
        load_best_model_at_end = True

    )

    # 트레이너 초기화 및 훈련
    trainer = Trainer(
        model = pretrained_model,
        args = training_args,
        train_dataset = answer_train_dataset_fold,
        eval_dataset = answer_val_dataset_fold,
        callbacks = [EarlyStoppingCallback(patience = 3)]
    )

    trainer.train()

#모델 성능 테스트
trainer = Trainer(
    model = pretrained_model,
    compute_metrics = compute_metrics,
)

trainer.predict(answer_test_dataset)

#google drive에 학습된 모델을 저장
save_path = f'/content/drive/My Drive/Finetuned_Model_judge_answer_{n_splits}fold'

pretrained_model.save_pretrained(save_path)
pretrained_tokenizer.save_pretrained(save_path)

"""## 요약 모델 테스트"""



"""## 임베딩 생성 모델 테스트"""

