# -*- coding: utf-8 -*-
"""BP_judge_question_KcBERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1me1_e-tzBq3kLtjNsQm9nXc9laWfU3U-
"""

#한국 버블 커뮤니티 오픈톡방 대화 중 질문을 선별하는 코드(로컬)

!pip install soynlp
!pip install datasets
!pip install kss
!pip install accelerate -U
import accelerate
import random
import pandas as pd
import numpy as np
import re
import os
import tensorflow as tf
import urllib.request
import torch
import torch.nn as nn
from kss import Kss
from transformers.modeling_outputs import SequenceClassifierOutput
from soynlp.word import WordExtractor
from soynlp.tokenizer import LTokenizer, MaxScoreTokenizer
from tqdm import tqdm
from torch.nn.functional import cross_entropy
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.preprocessing.sequence import pad_sequences
from transformers import AutoTokenizer, RobertaTokenizerFast, RobertaForSequenceClassification, TextClassificationPipeline, Trainer, TrainingArguments, RobertaConfig, shape_list, BertTokenizer, TFBertModel, RobertaModel, RobertaTokenizer, TrainerCallback, pipeline, BertForMaskedLM, BertConfig, BertForSequenceClassification
from datasets import Dataset, load_dataset, ClassLabel
from sklearn.model_selection import StratifiedKFold, train_test_split
from google.colab import drive
drive.mount('/content/drive')

#Sequence classification으로 훈련된 모델 로드
finetuned_model_load_path = '/content/drive/My Drive/Finetuned_Model_judge_question'

finetuned_tokenizer = AutoTokenizer.from_pretrained(finetuned_model_load_path)
finetuned_model = BertForSequenceClassification.from_pretrained(finetuned_model_load_path)

#카카오톡 대화내용 불러오고 데이터프레임으로 받기
file_path = '/content/drive/My Drive/talk_preprocess_result_short.xlsx'

df = pd.read_excel(file_path)

# Trainer가 알아서 gpu를 감지하고 모델과 데이터를 gpu로 옮겨줌
#GPU 사용 코드
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#파이프라인을 사용하는 경우 GPU로 모델을 옮겨준 뒤 파이프라인에 모델을 넣어줘야 함
finetuned_model.to(device)

#데이터 처리 Pipeline 설정, top_k를 1로 설정시 최대로 측정된 label만 출력됨.
#텍스트 처리해보니 시퀀스 길이가 539인 행 존재 -> truncation 진행, 최대 길이는 512
text_classifier = TextClassificationPipeline(
    tokenizer=finetuned_tokenizer,
    model=finetuned_model,
    top_k = 1,
    truncation = True,
    batch_size = 128,
    device = device
)

# #결측치 제거
# df['text'] = df['text'].dropna(axis = 0)

# #결측치 검증
# print(type(text[:5]))
# print(all(isinstance(item, str) for item in text))
# for i, item in enumerate(text):
#   if not isinstance(item, str):
#     print(f"Index {i}: {item} (Type: {type(item)})")

#결측치 제거
df = df.replace('None', np.nan).dropna(axis = 0)

#모델로 prediction_labels 리스트 생성.
text = df['text'].to_list()

#tqdm으로 진행률 표시
predictions= []
for i in tqdm(range(0, len(text), 128), desc='Classifying'):
    batch = text[i:i+128]
    predictions.extend(text_classifier(batch))

# predictions = text_classifier(text)
prediction_labels = ['question' if prediction[0]['label'] == 'LABEL_1' else 'not question' for prediction in predictions]

#토큰화 길이가 7 이하인 텍스트는 question이 아니라 answer로 수정.
for i in tqdm(range(len(text)), desc = 'Token length standard'):
  if len(finetuned_tokenizer(text[i])['input_ids']) < 7:
    prediction_labels[i] = 'not question'

#Dataframe에 새 열로 추가.
df['label'] = prediction_labels

print(df)
print(df[df['label'] == 'question'])

#.csv 파일로 google drive에 저장
#illegalcharactererror에 의해 .xlsx 대신 .csv로 저장
save_path = '/content/drive/My Drive/judge_question_result_short_KcBERT.csv'

df.to_csv(save_path)

