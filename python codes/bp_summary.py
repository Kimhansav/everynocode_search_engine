# -*- coding: utf-8 -*-
"""BP_summary.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14MGoZDTqBTAe9v8FQ893IX2vYZMXVMjP
"""

#한국 버블 커뮤니티 오픈톡방 대화의 질-답, 모두의노코드 질-답+댓 각각을 요약해 저장하는 코드(로컬)

!pip install datasets

import pandas as pd
import numpy as np
import re
import os
import torch
import tensorflow as tf
import urllib.request
import matplotlib.pyplot as plt
from datasets import Dataset, load_dataset
from google.colab import drive
from sklearn.model_selection import StratifiedKFold, train_test_split
from torch.utils.data import DataLoader
from tqdm import tqdm
from transformers import shape_list, BertTokenizer, TFBertModel, PreTrainedTokenizerFast, BartForConditionalGeneration
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.preprocessing.sequence import pad_sequences
drive.mount('/content/drive')

# Load Model and Tokenizer
tokenizer = PreTrainedTokenizerFast.from_pretrained("EbanLee/kobart-summary-v3")
model = BartForConditionalGeneration.from_pretrained("EbanLee/kobart-summary-v3")

#카카오톡 질답세션 불러오고 Dataset으로 받기
file_path = '/content/drive/My Drive/judge_answer_result_KcBERT_short_questiontrained.csv'

df1 = pd.read_csv(file_path, index_col = 0)

#커뮤니티 데이터와 결합하기 위해 url 열 추가
df1['url'] = ['.'] * len(df1)

# dataset = load_dataset("csv", data_files=file_path, column_names=["question", "answer", "questioner", "respondent", "date", "url"])

print(df1)

#모두의노코드 질-답+댓 세션 불러오고 Dataset으로 받기
file_path = '/content/drive/My Drive/community_qna_preprocessed.xlsx'

df2 = pd.read_excel(file_path, index_col = 0)

#카카오톡 데이터와 결합하기 위해 답변 작성자 열 추가
#json 데이터는 nan을 처리 못한다 !!!
df2['respondent'] = ['.'] * len(df2)

#Slug가 존재하는 경우만 질문 원글로 판별
df2 = df2[pd.isna(df2['Slug']) == False]
#답변이 달리지 않은 질문글은 제외, 추후 댓글까지 활용 예정
df2 = df2[pd.isna(df2['답변']) == False]

#필요없는 열 삭제
df2 = df2.drop(['댓글', '관련 링크', '제목', '첨부 파일', 'Slug'], axis = 1)

#카카오톡 데이터와 결합하기 위해 열 이름 변경
df2 = df2.rename(columns = {'내용' : 'question', '답변' : 'answer', '작성일' : 'date', '작성자' : 'questioner', '게시글 링크' : 'url'})

# dataset = load_dataset("csv", data_files = file_path, column_names = ["question", "answer", "questioner", "respondent", "date", "url"])

print(df2)

print(type(df1.iloc[2]['date']))

# df2['date'] = df2['date'].astype(str)

print(type(df2.iloc[3]['date']))

# #두 데이터프레임 합치고 Dataset으로 변환하기
df = pd.concat([df1, df2], ignore_index = True)
print(df)
dataset = Dataset.from_pandas(df)

print(dataset)

# Ensure the classifier is using GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model.to(device)

# # 데이터셋 전처리 (예제)
# texts = ["This is a long text that needs to be summarized.", "Here's another text for summarization."]
# tokenized_inputs = tokenizer(texts, padding="longest", truncation=True, return_tensors="pt", max_length=1024)

# print(tokenized_inputs)

# 데이터셋 질문 전처리 함수, Dataset 열을 인자로 받는다.
def preprocess_question(examples):
    return tokenizer(examples['question'], return_tensors = "pt", truncation=True, padding="max_length", max_length=1026)

#토큰화된 질문 길이 통계량 확인
# Encoding question
tokenized_question_inputs = tokenizer(df['question'].to_list())

# 토큰화된 질문에서 각 질문의 길이 계산
lengths = [len(input_id) for input_id in tokenized_question_inputs['input_ids']]

# 최대 길이를 갖는 질문의 인덱스 찾기
max_length_index = [index for index, length in enumerate(lengths) if length == 972]

# 해당 인덱스의 원본 질문 출력
for index in max_length_index:
    print(df['question'].iloc[index])

print('토큰화된 질문의 최대 길이 :', max(len(sample) for sample in tokenized_question_inputs['input_ids']))
print('토큰화된 질문의 평균 길이 :', sum(map(len, tokenized_question_inputs['input_ids']))/len(tokenized_question_inputs['input_ids']))

plt.hist([len(sample) for sample in tokenized_question_inputs['input_ids']], bins = 100)
plt.xlabel('length of samples')
plt.ylabel('number of samples')
plt.show()

print(len(tokenizer('버블 softr 웹플로우 아임웹 글라이드는 언급되는데 워드프레스는 노코드에 언급이 없어서 버블과 워드프레스는')['input_ids']))

#데이터를 gpu로 이동시킬 때 DataLoader에서 사용되는 collate_fn 함수
def collate_fn(batch):
    input_ids = torch.stack([torch.tensor(item['input_ids'], dtype = torch.long) for item in batch])
    attention_mask = torch.stack([torch.tensor(item['attention_mask'], dtype = torch.long) for item in batch])
    return {"input_ids": input_ids, "attention_mask": attention_mask}

#모델로 question summary 리스트 생성
question_summary = []

# Encoding question
question_text_dict = {"question" : dataset["question"][1:]} #dataset["question"]의 첫 요소인 'question'삭제
question_text = Dataset.from_dict(question_text_dict)
tokenized_question_inputs = question_text.map(preprocess_question, num_proc = 4, batched = True, batch_size = 128)

#데이터를 gpu로 이동시킬 때 DataLoader에서 사용되는 collate_fn 함수
def collate_fn(batch):
    input_ids = torch.stack([torch.tensor(item['input_ids'], dtype = torch.long) for item in batch])
    attention_mask = torch.stack([torch.tensor(item['attention_mask'], dtype = torch.long) for item in batch])
    return {"input_ids": input_ids, "attention_mask": attention_mask}

loader = DataLoader(tokenized_question_inputs, batch_size=16, collate_fn=collate_fn)

# Question summary ids 생성,배치 단위
for batch in loader:
  #배치 단위로 loader에서 gpu로 데이터 전송
  input_ids = batch['input_ids'].to(device)
  attention_mask = batch['attention_mask'].to(device)

  question_summary_ids = model.generate(
    input_ids = input_ids,
    attention_mask = attention_mask,
    bos_token_id = model.config.bos_token_id,
    eos_token_id = model.config.eos_token_id,
    length_penalty = 1.0,
    max_length = 100,
    min_length = 5,
    num_beams = 6,
    repetition_penalty = 1.5,
    no_repeat_ngram_size = 3,
  )
  #생성한 Question summary ids를 다시 tokenizer가 있는 cpu로 이동
  question_summary_ids = question_summary_ids.cpu()

  #배치마다 answer_summary_ids를 생성하기 때문에 cpu로 옮겨진 answer_summary_ids 배치 1개를 answer_summary 리스트에 옮기기
  question_summary.append(question_summary_ids)

# Decoding question summary (list)
qsum = []

#배치 단위로 decode 처리
for question_summary_result in question_summary:
  qsum_batch = [tokenizer.decode(question_summary_result[i], skip_special_tokens = True) for i in range(len(question_summary_result))]
  qsum.append(qsum_batch)

#배치 풀기
qsum = sum(qsum, [])
df_qsum = pd.DataFrame(qsum)

print(df_qsum)

# 데이터셋 답변 전처리 함수, Dataset 열을 인자로 받는다. 질문 전처리 함수와 공통적으로 사용하지 않은 이유는 examples['answer']형태로 입력하지 않으면 오류가 발생한다.
def preprocess_answer(examples):
    return tokenizer(examples['answer'], return_tensors = "pt", truncation=True, padding="max_length", max_length=1026)

#토큰화된 답변 길이 통계량 확인
# Encoding answer
tokenized_answer_inputs = tokenizer(df['answer'].to_list())

print('토큰화된 답변의 최대 길이 :', max(len(sample) for sample in tokenized_answer_inputs['input_ids']))
print('토큰화된 답변의 평균 길이 :', sum(map(len, tokenized_answer_inputs['input_ids']))/len(tokenized_answer_inputs['input_ids']))

plt.hist([len(sample) for sample in tokenized_answer_inputs['input_ids']], bins = 100)
plt.xlabel('length of samples')
plt.ylabel('number of samples')
plt.show()

#모델로 answer summary 리스트 생성
answer_summary = []

# Encoding answer
answer_text_dict = {"answer" : dataset["answer"][1:]} #dataset["answer"]의 첫 요소인 'answer'삭제
answer_text = Dataset.from_dict(answer_text_dict)
tokenized_answer_inputs = answer_text.map(preprocess_answer, num_proc = 4, batched = True, batch_size = 128)

loader = DataLoader(tokenized_answer_inputs, batch_size=16, collate_fn=collate_fn)

# Answer summary ids 생성,배치 단위, question때와 다른 하이퍼파라미터 필요(요약길이)
for batch in loader:
  #배치 단위로 loader에서 gpu로 데이터 전송
  input_ids = batch['input_ids'].to(device)
  attention_mask = batch['attention_mask'].to(device)

  answer_summary_ids = model.generate(
    input_ids = input_ids,
    attention_mask = attention_mask,
    bos_token_id = model.config.bos_token_id,
    eos_token_id = model.config.eos_token_id,
    length_penalty = 1.0,
    max_length = 200,
    min_length = 5,
    num_beams = 6,
    repetition_penalty = 1.5,
    no_repeat_ngram_size = 3,
  )

  #생성한 Answer summary ids를 다시 tokenizer가 있는 cpu로 이동
  answer_summary_ids = answer_summary_ids.cpu()

  #배치마다 answer_summary_ids를 생성하기 때문에 cpu로 옮겨진 answer_summary_ids 배치 1개를 answer_summary 리스트에 옮기기
  answer_summary.append(answer_summary_ids)

# Decoding question summary (list)
asum = []

#배치 단위로 decode 처리
for answer_summary_result in answer_summary:
  asum_batch = [tokenizer.decode(answer_summary_result[i], skip_special_tokens = True) for i in range(len(answer_summary_result))]
  asum.append(asum_batch)

#배치 풀기
asum = sum(asum, [])
df_asum = pd.DataFrame(asum)

print(df_asum)

#데이터셋에 새로운 열 추가
print(len(dataset))

dataset_dict = {'question' : dataset['question'][1:], 'answer' : dataset['answer'][1:], 'question_summary' : qsum, 'answer_summary' : asum, 'question_withurl' : dataset['question_withurl'][1:], 'answer_withurl' : dataset['answer_withurl'][1:], 'questioner' : dataset['questioner'][1:], 'respondent' : dataset['respondent'][1:], 'date' : dataset['date'][1:], 'url' : dataset['url'][1:]}
dataset = Dataset.from_dict(dataset_dict)

print(dataset)
# dataset.add_column("qsum", qsum)
# dataset.add_column("asum", asum)

#.csv 파일로 google drive에 저장
save_path = '/content/drive/My Drive/summary_result_short.csv'

dataset.to_csv(save_path)

