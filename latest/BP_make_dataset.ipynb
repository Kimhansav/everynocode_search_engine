{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kimhansav/everynocode_search_engine/blob/main/BP_make_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#KcBERT 학습용 데이터셋을 제작하는 코드(로컬)\n",
        "#제작할 데이터셋은 pretraining용 데이터셋(soynlp 학습용 데이터), finetuning용 데이터셋(next sentence prediction, text classification)"
      ],
      "metadata": {
        "id": "dvmRQmeXNwrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8yZcJ5xNqY6",
        "outputId": "39642cdd-7879-430b-aa3f-8dfe1c0b9876"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kss\n",
            "  Downloading kss-6.0.4.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting emoji==1.2.0 (from kss)\n",
            "  Downloading emoji-1.2.0-py3-none-any.whl (131 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.3/131.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pecab (from kss)\n",
            "  Downloading pecab-1.0.8.tar.gz (26.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.4/26.4 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from kss) (3.3)\n",
            "Collecting jamo (from kss)\n",
            "  Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Collecting hangul-jamo (from kss)\n",
            "  Downloading hangul_jamo-1.0.1-py3-none-any.whl (4.4 kB)\n",
            "Collecting tossi (from kss)\n",
            "  Downloading tossi-0.3.1.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting distance (from kss)\n",
            "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyyaml==6.0 (from kss)\n",
            "  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unidecode (from kss)\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cmudict (from kss)\n",
            "  Downloading cmudict-1.0.23-py3-none-any.whl (939 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.4/939.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting koparadigm (from kss)\n",
            "  Downloading koparadigm-0.10.0-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kollocate (from kss)\n",
            "  Downloading kollocate-0.0.2-py3-none-any.whl (72.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.2/72.2 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bs4 (from kss)\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from kss) (1.25.2)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from kss) (7.4.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from kss) (1.11.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4->kss) (4.12.3)\n",
            "Requirement already satisfied: importlib-metadata>=5 in /usr/local/lib/python3.10/dist-packages (from cmudict->kss) (7.1.0)\n",
            "Requirement already satisfied: importlib-resources>=5 in /usr/local/lib/python3.10/dist-packages (from cmudict->kss) (6.4.0)\n",
            "Collecting whoosh (from kollocate->kss)\n",
            "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.8/468.8 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xlrd==1.2.0 (from koparadigm->kss)\n",
            "  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (14.0.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (2023.12.25)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->kss) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest->kss) (24.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->kss) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->kss) (1.2.1)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->kss) (2.0.1)\n",
            "Requirement already satisfied: bidict in /usr/local/lib/python3.10/dist-packages (from tossi->kss) (0.23.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tossi->kss) (1.16.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=5->cmudict->kss) (3.18.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4->kss) (2.5)\n",
            "Building wheels for collected packages: kss, distance, pecab, tossi\n",
            "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kss: filename=kss-6.0.4-cp310-cp310-linux_x86_64.whl size=1446489 sha256=ad8e017e82ece2d4ba976bbbdd1b37b4ca77abde5bc0a02a48d48c9b8da27a69\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/70/d5/c9308346829b1eb9e7267d74696919d2453aee6ce350f98b3b\n",
            "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16258 sha256=73a6343c6fb07fc28714e48dcce72cb3f7d788138064767483a8fbcde1a12eb8\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/bb/de/f71bf63559ea9a921059a5405806f7ff6ed612a9231c4a9309\n",
            "  Building wheel for pecab (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pecab: filename=pecab-1.0.8-py3-none-any.whl size=26646664 sha256=a3e3c5d4832d4f1e05f6f6f8ef9d73b49dbf620d34b5de6079802f719ac8973e\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/6f/b4/ab61b8863d7d8b1409def8ae31adcaa089fa91b8d022ec309d\n",
            "  Building wheel for tossi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tossi: filename=tossi-0.3.1-py3-none-any.whl size=12130 sha256=26ea871cb51f085783a3609ae6aad05f8b386708ae89d7f54a456f6f10f5901f\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/18/60/1094a6fe93c8063efcd3e6700d09328216682e495a3c51af9f\n",
            "Successfully built kss distance pecab tossi\n",
            "Installing collected packages: whoosh, jamo, hangul-jamo, emoji, distance, xlrd, unidecode, tossi, pyyaml, kollocate, pecab, koparadigm, cmudict, bs4, kss\n",
            "  Attempting uninstall: xlrd\n",
            "    Found existing installation: xlrd 2.0.1\n",
            "    Uninstalling xlrd-2.0.1:\n",
            "      Successfully uninstalled xlrd-2.0.1\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0.1\n",
            "    Uninstalling PyYAML-6.0.1:\n",
            "      Successfully uninstalled PyYAML-6.0.1\n",
            "Successfully installed bs4-0.0.2 cmudict-1.0.23 distance-0.1.3 emoji-1.2.0 hangul-jamo-1.0.1 jamo-0.4.1 kollocate-0.0.2 koparadigm-0.10.0 kss-6.0.4 pecab-1.0.8 pyyaml-6.0 tossi-0.3.1 unidecode-1.3.8 whoosh-2.7.4 xlrd-1.2.0\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed datasets-2.19.0 dill-0.3.8 huggingface-hub-0.23.0 multiprocess-0.70.16 xxhash-3.4.1\n",
            "Collecting soynlp\n",
            "  Downloading soynlp-0.0.493-py3-none-any.whl (416 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.8/416.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.10/dist-packages (from soynlp) (1.25.2)\n",
            "Requirement already satisfied: psutil>=5.0.1 in /usr/local/lib/python3.10/dist-packages (from soynlp) (5.9.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from soynlp) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from soynlp) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->soynlp) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->soynlp) (3.5.0)\n",
            "Installing collected packages: soynlp\n",
            "Successfully installed soynlp-0.0.493\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install kss\n",
        "!pip install datasets\n",
        "!pip install soynlp\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "import urllib.request\n",
        "from soynlp.normalizer import *\n",
        "from kss import Kss\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from transformers import AutoTokenizer, shape_list, TFBertModel, RobertaTokenizerFast, RobertaForSequenceClassification, TextClassificationPipeline, pipeline, BertTokenizer, BertForNextSentencePrediction, TrainingArguments, BertForSequenceClassification\n",
        "from datasets import Dataset, load_dataset, ClassLabel\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#전처리된 파일 로드(BP_spacing 건너뛰기)\n",
        "talk_save_path = '/content/drive/My Drive/talk_preprocess_result_short.xlsx'\n",
        "qna_path = '/content/drive/My Drive/community_qna_preprocessed.xlsx'\n",
        "all_contents_path = '/content/drive/My Drive/community_all_contents_preprocessed.xlsx'\n",
        "all_comments_path = '/content/drive/My Drive/community_all_comments_preprocessed.xlsx'\n",
        "\n",
        "df_talk = pd.read_excel(talk_save_path)\n",
        "df_qna = pd.read_excel(qna_path)\n",
        "df_all_contents = pd.read_excel(all_contents_path)\n",
        "df_all_comments = pd.read_excel(all_comments_path)"
      ],
      "metadata": {
        "id": "HPlZt8oJN3fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qna_path = '/content/drive/My Drive/community_qna_preprocessed.xlsx'\n",
        "df_qna = pd.read_excel(qna_path)"
      ],
      "metadata": {
        "id": "wWMi4ts7rDZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#직접 제작한 카카오톡 질문답변 데이터셋 로드\n",
        "talk_finetune_path = '/content/drive/My Drive/talk_finetune_dataset.xlsx'\n",
        "\n",
        "df_talk_finetune = pd.read_excel(talk_finetune_path, index_col = 0)"
      ],
      "metadata": {
        "id": "o2OuDponqe2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#직접 제작한 카카오톡 질문 데이터셋 로드\n",
        "talk_question_finetune_path = '/content/drive/My Drive/talk_finetune_question_dataset.xlsx'\n",
        "\n",
        "df_talk_finetune_question = pd.read_excel(talk_question_finetune_path, index_col = 0)"
      ],
      "metadata": {
        "id": "hNQytfRqPcrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Soyspacing 학습시킨 후에 띄어쓰기 교정"
      ],
      "metadata": {
        "id": "u3whgqGwAR1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#일단 보류"
      ],
      "metadata": {
        "id": "2g_LiWn7AQ6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pretrain 데이터셋, 제공받은 모든 데이터 사용\n"
      ],
      "metadata": {
        "id": "s_4qB1f1cH1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_sentences = Kss('split_sentences')"
      ],
      "metadata": {
        "id": "fhrgE6AOkwL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#kss로 리스트 안에 있는 여러 문장으로 이루어진 글을 문장 단위로 분리하는 함수\n",
        "def separate_to_sentences_and_deduplicate(DataFrame, columns):\n",
        "    sentences_list = []\n",
        "    for column in columns: #데이터프레임의 각 열에 대해서 처리\n",
        "        text_list = DataFrame[column].to_list()\n",
        "        sentences_list.append(split_sentences(text_list))\n",
        "    return list(set(sum([lists for lists in sentences_list[0]], [])))"
      ],
      "metadata": {
        "id": "Bi-xF2jmkJe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터프레임의 열들을 리스트로 변환\n",
        "talk_list = separate_to_sentences_and_deduplicate(df_talk, ['text'])"
      ],
      "metadata": {
        "id": "UbFUcGHbScGp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aef0a4f3-10b2-4419-da16-108b7a834ea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Oh! You have mecab in your environment. Kss will take this as a backend! :D\n",
            "\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터프레임의 NaN 대신 '' 채워넣기\n",
        "df_qna.fillna('', inplace = True)\n",
        "df_all_contents.fillna('', inplace = True)\n",
        "df_all_comments.fillna('', inplace = True)"
      ],
      "metadata": {
        "id": "9qiuANBMmgMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qna_list = separate_to_sentences_and_deduplicate(df_qna, ['내용', '답변', '댓글'])"
      ],
      "metadata": {
        "id": "uA1jor5mgkMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_contents_list = separate_to_sentences_and_deduplicate(df_all_contents, ['내용', '답변', '댓글'])"
      ],
      "metadata": {
        "id": "Dauyn3R_mhlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_comments_list = separate_to_sentences_and_deduplicate(df_all_comments, ['내용', '대댓글', '댓글'])\n",
        "print(all_comments_list)"
      ],
      "metadata": {
        "id": "ny2BfSH2mnpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#하나의 리스트로 결합\n",
        "result_list = talk_list + qna_list + all_contents_list + all_comments_list\n",
        "print(result_list[:10])\n",
        "print(len(result_list))\n",
        "df_result = pd.DataFrame(result_list, columns = ['text'])\n",
        "print(df_result)\n",
        "print(len(df_result))"
      ],
      "metadata": {
        "id": "jqlzQ3J7jqCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#전체 데이터에 대해 soynlp.normalize 적용?\n",
        "#일단 보류"
      ],
      "metadata": {
        "id": "gBufYpDGmn4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#.csv 파일로 저장(.xlsx로 저장 시 illegalcharactererror 발생)\n",
        "pretrain_data_save_path = '/content/drive/My Drive/KcBERT_pretrain_dataset_short.csv'\n",
        "\n",
        "df_result.to_csv(pretrain_data_save_path)"
      ],
      "metadata": {
        "id": "kt4_LDAkb4fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Finetune 데이터셋(질문-답변)\n"
      ],
      "metadata": {
        "id": "Lxsu8xXKcMJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#slug가 있다면 ['내용'] == 질문\n",
        "df_question_answer = df_qna[pd.isna(df_qna['Slug']) == False]\n",
        "df_question_answer = df_question_answer[pd.isna(df_question_answer['답변']) == False]\n",
        "\n",
        "print(df_question_answer)\n",
        "print(len(df_question_answer))\n",
        "\n",
        "print(df_question_answer.iloc[0])\n",
        "print(df_question_answer.iloc[1]['답변'])"
      ],
      "metadata": {
        "id": "-cn_1Zvkltdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "질문 + 답변에다가 답변에 달린 댓글들까지 입력 데이터로 처리할 경우 퀄리티가 더 좋아질 수 있지만, max_length를 초과하지 않는 방향에 따라 답변 데이터만 이용, 나중에 개선방안 찾아보기"
      ],
      "metadata": {
        "id": "zwVSXoH7tcCC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "community_qna_preprocessed 데이터에서 이미지 파일을 base64 인코딩한 텍스트가 섞여 있는데, 데이터 파일 추출 과정에서 직접 제거하는 게 좋을 것 같음."
      ],
      "metadata": {
        "id": "b_y534xjvGFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#커뮤니티 파일을 가공해서 finetune 데이터셋을 제작\n",
        "finetune_dict = {}\n",
        "\n",
        "#slug가 있다면 ['내용'] == 질문\n",
        "df_question_answer = df_qna[df_qna['Slug'] != '']\n",
        "df_question_answer = df_question_answer[df_question_answer['답변'] != '']\n",
        "\n",
        "print(df_question_answer.iloc[2])\n",
        "\n",
        "#['답변']의 텍스트를 ' , ' 로 구분 후 질문에 하나씩 쌓기\n",
        "sent1_list, sent2_list, label_list, index_list = [], [], [], []\n",
        "for i in range(len(df_question_answer)):\n",
        "  row = df_question_answer.iloc[i]\n",
        "  question_text = row['내용']\n",
        "  answer_text_list = row['답변'].split(' , ')\n",
        "  for j in range(len(answer_text_list)):\n",
        "    if answer_text_list[j] != '':\n",
        "      sent1_list.append(question_text + ''.join(answer_text_list[:j]))\n",
        "      sent2_list.append(answer_text_list[j])\n",
        "      label_list.append(1)\n",
        "      index_list.append(i)\n",
        "\n",
        "finetune_dict = {'sent1' : sent1_list, 'sent2' : sent2_list, 'label' : label_list, 'sent2_index' : index_list}\n",
        "\n",
        "df_community_finetune = pd.DataFrame.from_dict(finetune_dict)\n",
        "print(df_community_finetune)"
      ],
      "metadata": {
        "id": "DJu7dDaXcWyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#파인튜닝 데이터셋에서 n배의 negative sample을 생성하는 코드\n",
        "def create_negative_sample(DataFrame, n):\n",
        "  DataFrame_copy = DataFrame.copy() #원본 데이터셋을 변화시키지 않기 위해 deep copy\n",
        "  length = len(DataFrame_copy)\n",
        "  for i in range(length):\n",
        "    row = DataFrame_copy.iloc[i]\n",
        "    negative_candidate_ids = list(set(DataFrame_copy['sent2_index'].tolist())) #.remove(row['sent2_index']) #해당 행의 sent2를 negative sample 후보군에서 제외\n",
        "    negative_candidate_ids.remove(row['sent2_index'])\n",
        "    negative_sample_ids = list(np.random.choice(negative_candidate_ids, n, replace = False))\n",
        "    for idx in negative_sample_ids:\n",
        "      new_row = pd.Series({'sent1' : row['sent1'], 'sent2' : np.random.choice(DataFrame_copy[DataFrame_copy['sent2_index'] == idx]['sent2'].tolist(), 1)[0], 'label' : 0, 'sent2_index' : idx}) #카톡 데이터는 idx 중복이 없기 때문에 랜덤추출로 1개를 뽑아도 달라지는 건 없고, 커뮤니티 데이터의 경우 idx 중복이 있기 때문에 해당 idx를 가진 sent2 중 하나를 뽑아야 함.\n",
        "      DataFrame_copy = pd.concat([DataFrame_copy, new_row.to_frame().T], ignore_index = True)\n",
        "  return DataFrame_copy"
      ],
      "metadata": {
        "id": "1zADZ7nnwtq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#커뮤니티 파인튜닝 데이터셋에서 negative sample 생성\n",
        "df_community_finetune_negative_sampled = create_negative_sample(df_community_finetune, 7)\n",
        "# df_community_finetune_negative_sampled = df_community_finetune_negative_sampled.drop('sent2_index', axis = 1)\n",
        "print(len(df_community_finetune_negative_sampled))\n",
        "print(df_community_finetune_negative_sampled)"
      ],
      "metadata": {
        "id": "4lHYLPSnl7H1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_community_finetune_negative_sampled[df_community_finetune_negative_sampled['sent2'] == ''])\n",
        "print(df_community_finetune[df_community_finetune['sent2_index'] == 13])"
      ],
      "metadata": {
        "id": "rdree1ycnI0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_talk_finetune)"
      ],
      "metadata": {
        "id": "kqWsvPrvLzQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#카카오톡 파인튜닝 데이터셋에서 negative sample 생성\n",
        "df_talk_finetune_negative_sampled = create_negative_sample(df_talk_finetune, 7)\n",
        "df_talk_finetune_negative_sampled = df_talk_finetune_negative_sampled.drop('sent2_index', axis = 1)\n",
        "print(len(df_talk_finetune_negative_sampled))\n",
        "print(df_talk_finetune_negative_sampled)"
      ],
      "metadata": {
        "id": "HgSubqZCmHbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_talk_finetune_negative_sampled[df_talk_finetune_negative_sampled['sent2'] == ''])"
      ],
      "metadata": {
        "id": "M_60G8dnpFbk",
        "outputId": "509591ea-bfd2-48e6-9a0c-e8ce232ee20d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [sent1, sent2, label]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#두 파인튜닝 데이터셋을 하나로 합치기\n",
        "df_KcBERT_finetune = pd.concat([df_community_finetune_negative_sampled, df_talk_finetune_negative_sampled], ignore_index = True)\n",
        "print(len(df_KcBERT_finetune))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3srQkz3l8LxD",
        "outputId": "4b1ca4f2-2a41-486f-b5b1-975e7a016ae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#.xlsx 파일로 저장\n",
        "finetune_data_save_path = '/content/drive/My Drive/KcBERT_finetune_dataset_answer.xlsx'\n",
        "\n",
        "df_KcBERT_finetune.to_excel(finetune_data_save_path)"
      ],
      "metadata": {
        "id": "pjnhY-llcGyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Finetune 데이터셋(질문)"
      ],
      "metadata": {
        "id": "f5uY9km_PXZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "왜 빌더로그 게시글 요소들을 세 부분으로 나눠야 하는가?\n",
        "1. positive sample, negative sample 비율 1대1이 되도록 만들기\n",
        "2. 사용할 RobertaForSequenceClassification 아키텍처를 보면 (position_embeddings): Embedding(514, 768, padding_idx=1)이 포함되어 있다. negative sample에 해당하는 빌더로그 게시글만 길이가 길다면 이러한 정보까지 학습할 여지가 있다."
      ],
      "metadata": {
        "id": "KZqsa98KQYH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "왜 tokenizer()로 바로 시퀀스 길이를 구하지 않고 원본 리스트부터 시작해서 평균 길이를 구하는가\n",
        "->\n",
        "\n",
        "인자 중 max_length가 존재하기 때문에 어차피 길이를 초과하는 데이터는 truncation이 진행된다.\n",
        "\n",
        "그렇기 때문에 데이터 평균 길이를 비교하려면 원본 리스트를 사용해야 한다."
      ],
      "metadata": {
        "id": "GcIx9fAQQaNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#토큰화를 위한 pretrained model 로드\n",
        "pretrained_model_path = '/content/drive/My Drive/Pretrained_Model'\n",
        "\n",
        "pretrained_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_path)\n",
        "pretrained_model = BertForSequenceClassification.from_pretrained(pretrained_model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5jfCFPKHRE6",
        "outputId": "0cecbfdf-12b5-486c-afaa-4356b2af64a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/drive/My Drive/Pretrained_Model and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#제작한 데이터셋의 결측치 제거\n",
        "df_talk_finetune_question = df_talk_finetune_question[(pd.notna(df_talk_finetune_question['text'])) & (pd.notna(df_talk_finetune_question['label']))]\n",
        "print(df_talk_finetune_question)\n",
        "df_talk_finetune_question_positive = df_talk_finetune_question[df_talk_finetune_question['label'] == 1]\n",
        "print(df_talk_finetune_question_positive)"
      ],
      "metadata": {
        "id": "YFxBJpV6Buen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터셋의 tokenized data의 길이를 구하는 함수\n",
        "def tokenized_sequence_length(text):\n",
        "  #kss 모듈로 글을 문장으로 분리\n",
        "  original_sentences = split_sentences(text)\n",
        "  index_sentences = pretrained_tokenizer(original_sentences)['input_ids']\n",
        "  index_sentences_filtered = [sentence[1:-1] for sentence in index_sentences] #cls_token과 sep_token 제외\n",
        "  total_length = sum(len(s) for s in index_sentences_filtered)\n",
        "  return index_sentences_filtered, total_length"
      ],
      "metadata": {
        "id": "HkBFUaG8QhwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터셋에서 각 요소를 n등분하는 함수\n",
        "def divide_text_to_number(text, number):\n",
        "  #kss 모듈로 문장으로 분리\n",
        "  index_sentences, total_length = tokenized_sequence_length(text)\n",
        "  target_length = total_length / number\n",
        "\n",
        "  # 세 부분으로 분할\n",
        "  segments = []\n",
        "  current_segment = []\n",
        "  current_length = 0\n",
        "\n",
        "  #마지막 세그먼트 추가를 위해 사용하는 인덱스 값\n",
        "  sentence_checkpoint = 0\n",
        "\n",
        "  for index, sentence in enumerate(index_sentences):\n",
        "      if current_length + len(sentence) > target_length and current_segment:\n",
        "          segments.append(sum(current_segment, []))\n",
        "          current_segment = [sentence]\n",
        "          current_length = len(sentence)\n",
        "          if len(segments) == number - 1:  # 이미 (목표 세그먼트 개수 - 1)개 세그먼트를 만들었다면 나머지는 모두 마지막 세그먼트에 추가\n",
        "              sentence_checkpoint = index #current_segment에 추가해둔 문장을 인덱싱\n",
        "              break\n",
        "      else:\n",
        "          current_segment.append(sentence)\n",
        "          current_length += len(sentence)\n",
        "\n",
        "  # 마지막 세그먼트 추가\n",
        "  segments.append(sum(current_segment + index_sentences[sentence_checkpoint + 1:], []))\n",
        "\n",
        "  return pretrained_tokenizer.batch_decode(segments)\n"
      ],
      "metadata": {
        "id": "bo2KDi3DQhyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BP_judge_question용 데이터 제작\n",
        "\n",
        "#slug가 있다면 질문, 없다면 답변, 이 중 질문을 positive sample로 설정\n",
        "question_list = df_qna[pd.isna(df_qna['Slug']) == False]['내용'].dropna(axis = 0).to_list()\n",
        "label_question = [1] * len(question_list)\n",
        "\n",
        "print(question_list)\n",
        "print('데이터셋의 전체 길이 :', len(question_list))\n",
        "\n",
        "#질문 평균 길이 확인(원본 텍스트)\n",
        "print('원본 텍스트 평균 길이 :', sum(len(question) for question in question_list) / len(question_list))\n",
        "\n",
        "#질문 평균 길이 확인(토큰화 결과)\n",
        "print('토큰화된 텍스트 평균 길이 :', sum(tokenized_sequence_length(text)[1] for text in question_list) / len(question_list))"
      ],
      "metadata": {
        "id": "8agmvEUOQh01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#답변을 negative sample로 설정\n",
        "answer_list = df_qna['답변'].dropna(axis = 0).to_list()\n",
        "label_answer = [0] * len(answer_list)\n",
        "\n",
        "print(answer_list)\n",
        "print(len(answer_list))\n",
        "\n",
        "#답변 평균 길이 확인(원본 텍스트)\n",
        "print('원본 텍스트 평균 길이 :', sum(len(answer) for answer in answer_list) / len(answer_list))\n",
        "\n",
        "#답변 평균 길이 확인(토큰화 결과)\n",
        "print('토큰화된 텍스트 평균 길이 :', sum(tokenized_sequence_length(text)[1] for text in answer_list) / len(answer_list))\n",
        "\n",
        "#시퀀스 길이 균형을 위해 각 답변 글을 n등분\n",
        "answer_list_divided = sum([divide_text_to_number(text, 2) for text in answer_list], [])\n",
        "\n",
        "#분할한 데이터의 개수에 맞춰서 레이블 설정\n",
        "label_answer = [0] * (len(answer_list_divided))\n",
        "\n",
        "#분리된 답변 글 평균 길이 확인(토큰화 결과)\n",
        "print('분리된 데이터셋에서 토큰화된 텍스트 평균 길이 :', sum(tokenized_sequence_length(text)[1] for text in answer_list_divided) / len(answer_list_divided))\n",
        "\n",
        "#분리된 데이터셋의 전체 길이\n",
        "print('분리된 데이터셋의 전체 길이 :', len(answer_list_divided))"
      ],
      "metadata": {
        "id": "mRgiLLBQQmX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#빌더로그 글을 negative sample로 설정\n",
        "builder_log_list = df_all_contents[df_all_contents['커뮤니티 타입'] == '빌더 로그']['내용'].dropna(axis = 0).to_list()\n",
        "\n",
        "print(builder_log_list)\n",
        "print(len(builder_log_list))\n",
        "\n",
        "#빌더로그 글 평균 길이 확인(원본 텍스트)\n",
        "print('원본 텍스트 평균 길이 :', sum(len(builder_log) for builder_log in builder_log_list) / len(builder_log_list))\n",
        "\n",
        "#빌더로그 글 평균 길이 확인(토큰화 결과)\n",
        "print('토큰화된 텍스트 평균 길이 :', sum(tokenized_sequence_length(text)[1] for text in builder_log_list) / len(builder_log_list))\n",
        "\n",
        "#시퀀스 길이 균형을 위해 각 빌더로그 글을 n등분\n",
        "builder_log_list_divided = sum([divide_text_to_number(text, 6) for text in builder_log_list], [])\n",
        "\n",
        "#분할한 데이터의 개수에 맞춰서 레이블 설정\n",
        "label_builder_log = [0] * (len(builder_log_list_divided))\n",
        "\n",
        "#분리된 빌더로그 글 평균 길이 확인(토큰화 결과)\n",
        "print('분리된 데이터셋에서 토큰화된 텍스트 평균 길이 :', sum(tokenized_sequence_length(text)[1] for text in builder_log_list_divided) / len(builder_log_list_divided))\n",
        "\n",
        "#분리된 데이터셋의 전체 길이\n",
        "print('분리된 데이터셋의 전체 길이 :', len(builder_log_list_divided))"
      ],
      "metadata": {
        "id": "OEC7TFEwQmaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#커뮤니티 글 데이터프레임 생성\n",
        "text = question_list + answer_list + builder_log_list_divided\n",
        "label = label_question + label_answer + label_builder_log\n",
        "\n",
        "df_judge_question = pd.DataFrame([text, label], index = ['text', 'label'])\n",
        "df_judge_question = df_judge_question.T\n",
        "df_judge_question = df_judge_question[pd.isna(df_judge_question['text']) == False]\n",
        "print(df_judge_question)\n"
      ],
      "metadata": {
        "id": "dIoGy8__Qmck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#직접 제작한 카카오톡 데이터프레임과 결합\n",
        "df_KcBERT_finetune_question = pd.concat([df_judge_question, df_talk_finetune_question], ignore_index = True)\n",
        "print(df_KcBERT_finetune_question)"
      ],
      "metadata": {
        "id": "XjDt_9Y3Qmet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#.xlsx 파일로 저장\n",
        "finetune_data_save_path = '/content/drive/My Drive/KcBERT_finetune_dataset_question.xlsx'\n",
        "\n",
        "df_KcBERT_finetune_question.to_excel(finetune_data_save_path)"
      ],
      "metadata": {
        "id": "eOEw77VuS13C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
