{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kimhansav/everynocode_search_engine/blob/main/BP_judge_answer_zeroshot_MoritzLaurer_mDeBERTa_v3_base_mnli_xnli_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wabey716zPNq"
      },
      "outputs": [],
      "source": [
        "#한국 버블 커뮤니티 오픈톡방 대화의 질문에 대한 답변을 선별하는 코드(zero-shot text classification)(MoritzLaurer/mDeBERTa-v3-base-mnli-xnli)(로컬)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6eFub4T38V1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from transformers import shape_list, BertTokenizer, TFBertModel\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "import urllib.request\n",
        "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification, TextClassificationPipeline, pipeline\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qs4N6sU7SefQ"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset, load_dataset\n",
        "import torch\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHdWohm7tup8"
      },
      "outputs": [],
      "source": [
        "#모델 불러오기\n",
        "HUGGINGFACE_MODEL_PATH = \"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\"\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\", model = HUGGINGFACE_MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-w6sLlB27mPW"
      },
      "outputs": [],
      "source": [
        "#모델 실험\n",
        "sequence_to_classify = \"Angela Merkel ist eine Politikerin in Deutschland und Vorsitzende der CDU\"\n",
        "candidate_labels = [\"politics\", \"economy\", \"entertainment\", \"environment\"]\n",
        "output = classifier(sequence_to_classify, candidate_labels, multi_label=False)\n",
        "print(output)\n",
        "print(output['labels'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWN3N25M0FdD"
      },
      "outputs": [],
      "source": [
        "#훈련 데이터 불러오기\n",
        "dataset_path = '/content/drive/My Drive/KakaoTalkChats-1.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcQirWwI0K1l"
      },
      "outputs": [],
      "source": [
        "#카카오톡 데이터 불러오기\n",
        "file_path = '/content/drive/My Drive/judge_question_result.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03vWAudThdM2"
      },
      "outputs": [],
      "source": [
        "#카카오톡 대화내용을 데이터프레임으로 받기\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "#질문 딕셔너리, 답변 목록 리스트(이중 리스트) 생성\n",
        "questions, answer_lists = {}, []\n",
        "\n",
        "#질문으로 판별된 텍스트를 새 데이터프레임으로 생성\n",
        "df_question = df[df['label'] == 'question']\n",
        "\n",
        "print(df_question)\n",
        "#새 데이터프레임의 index, text를 question 딕셔너리에 저장\n",
        "questions = {text : index for (index, text) in zip(df_question.index, df_question['text'])}\n",
        "print(questions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the classifier is using GPU\n",
        "device = 0 if torch.cuda.is_available() else -1  # Use GPU if available, otherwise CPU\n",
        "print(device)\n",
        "classifier = pipeline('zero-shot-classification', model=HUGGINGFACE_MODEL_PATH, device=device, batch_size = 64)"
      ],
      "metadata": {
        "id": "yyaMglos92s_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfFU5ST9Nyau"
      },
      "outputs": [],
      "source": [
        "#전체 질문에 대해서 각 질문 이후 30개의 텍스트에 대해서 판별, 이때 특정 텍스트가 답변이 맞으면 답변 리스트에 추가한 뒤 다음 텍스트 판별\n",
        "\n",
        "# Pre-fetch the necessary DataFrame data to minimize access within loops\n",
        "question_texts = {question: df.iloc[questions[question] + 1 : questions[question] + 6]['text'].tolist() for question in questions.keys()}\n",
        "\n",
        "answer_lists = []\n",
        "i = 0\n",
        "for question, texts in tqdm(question_texts.items(), desc=\"Processing questions\"):\n",
        "    answer = []\n",
        "    for text in texts: #나중에 숫자 커지면 texts 대신 tqdm(texts, desc=\"Classifying texts\", leave=False)\n",
        "        sequence_to_classify = question + ' '.join(answer + [text])\n",
        "        #동적 레이블\n",
        "        candidate_labels = [\"continuing answer\", \"not continuing answer\"] if answer else [\"question-answer pair\", \"not question-answer pair\"]\n",
        "\n",
        "        # Process the classification in one step\n",
        "        output = classifier(sequence_to_classify, candidate_labels, multi_label=False)\n",
        "        expected_label = \"continuing answer\" if answer else \"question-answer pair\"\n",
        "        if output['labels'][0] == expected_label:\n",
        "            answer.append(text)\n",
        "    answer_lists.append(answer) if len(answer) != 0 else answer_lists.append(['No answer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XinB54vVC31p"
      },
      "outputs": [],
      "source": [
        "print(answer_lists, end = '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpfHnw9R61mZ"
      },
      "outputs": [],
      "source": [
        "# #데이터프레임 가공\n",
        "# df_question.rename(columns = {'text' : 'question'})\n",
        "# df_question['answer'] = answer_lists\n",
        "\n",
        "data = {question : answer for question, answer in zip(questions.keys(), answer_lists)}\n",
        "print(data)\n",
        "data_result= [{'question' : key, 'answer' : value} for key, value in data.items()]\n",
        "df_result = pd.DataFrame(data = data_result, columns = ['question', 'answer'])\n",
        "print(df_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXT7YkGs1Ede"
      },
      "outputs": [],
      "source": [
        "#.csv 파일로 google drive에 저장\n",
        "csv_file_path = '/content/drive/My Drive/judge_answer_result_MoritzLaurer_mDeBERTa-v3-base-mnli-xnli.csv'\n",
        "\n",
        "df_result.to_csv(csv_file_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPJD9Hb+40SDFap/XRMOO/Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
