{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7d0ae957eafc473aa18f799141e7d06c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a33768c23f0845e59bcd4e49b81ae9dd",
              "IPY_MODEL_f149f4f343ba491cadd52d2acecbd362",
              "IPY_MODEL_d9859841d45b4ad1b26ea2f30a2f9dc5"
            ],
            "layout": "IPY_MODEL_e579426f57794038bf4f6e7015a7ae01"
          }
        },
        "a33768c23f0845e59bcd4e49b81ae9dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37aa6ad2a934476081326b3245f39016",
            "placeholder": "​",
            "style": "IPY_MODEL_24bca1ac22dd400983e87203f127a183",
            "value": "Map: 100%"
          }
        },
        "f149f4f343ba491cadd52d2acecbd362": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdad51b4c617401098b0ed8cffb26b1c",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_987b8c4e6ca74bc5b2f2b20d2383a568",
            "value": 612
          }
        },
        "d9859841d45b4ad1b26ea2f30a2f9dc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6af43eae04aa469d8d158db0affa86ba",
            "placeholder": "​",
            "style": "IPY_MODEL_ac3bc4c70ed4448398093c51a0f897fa",
            "value": " 612/612 [00:00&lt;00:00, 788.45 examples/s]"
          }
        },
        "e579426f57794038bf4f6e7015a7ae01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37aa6ad2a934476081326b3245f39016": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24bca1ac22dd400983e87203f127a183": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fdad51b4c617401098b0ed8cffb26b1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "987b8c4e6ca74bc5b2f2b20d2383a568": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6af43eae04aa469d8d158db0affa86ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac3bc4c70ed4448398093c51a0f897fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kimhansav/everynocode_search_engine/blob/main/BP_judge_question.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_iVynvry-Vk"
      },
      "outputs": [],
      "source": [
        "#한국 버블 커뮤니티 오픈톡방 대화 중 질문을 선별하는 코드(로컬)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install kss\n",
        "!pip install accelerate -U\n",
        "import accelerate\n",
        "import kss\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import urllib.request\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "from tqdm import tqdm\n",
        "from torch.nn.functional import cross_entropy\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification, TextClassificationPipeline, Trainer, TrainingArguments, RobertaConfig, shape_list, BertTokenizer, TFBertModel, RobertaModel, RobertaTokenizer, TrainerCallback\n",
        "from datasets import Dataset, load_dataset\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Sq0NdZXL39r1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델과 토크나이저 초기화\n",
        "HUGGINGFACE_MODEL_PATH = \"bespin-global/klue-roberta-small-3i4k-intent-classification\"\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(HUGGINGFACE_MODEL_PATH)\n",
        "pretrained_model = RobertaForSequenceClassification.from_pretrained(HUGGINGFACE_MODEL_PATH)"
      ],
      "metadata": {
        "id": "SIDCBpIKjyv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#출력 레이블 개수를 바꾸기 위해 출력층 하나 더 쌓기\n",
        "class ExtendedRobertaForSequenceClassification(nn.Module):\n",
        "    def __init__(self, pretrained_model):\n",
        "        super(ExtendedRobertaForSequenceClassification, self).__init__()\n",
        "        # 기존 파인튜닝 모델 로드\n",
        "        self.pretrained_model = pretrained_model\n",
        "        self.config = RobertaConfig(\n",
        "            _name_or_path = \"BP_judge_question\",\n",
        "            architectures = [\"RobertaForSequenceClassification\"],\n",
        "            attention_probs_dropout_prob = 0.1,\n",
        "            bos_token_id = 0,\n",
        "            eos_token_id = 2,\n",
        "            hidden_act = \"gelu\",\n",
        "            hidden_dropout_prob = 0.1,\n",
        "            hidden_size = 768,\n",
        "            id2label = {\"0\": \"not-question\", \"1\": \"question\"},\n",
        "            initializer_range = 0.02,\n",
        "            intermediate_size = 3072,\n",
        "            label2id = {\"question\": 1, \"not-question\": 0},\n",
        "            layer_norm_eps = 1e-05,\n",
        "            max_position_embeddings = 514,\n",
        "            model_type = \"roberta\",\n",
        "            num_attention_heads = 12,\n",
        "            num_hidden_layers = 6,\n",
        "            pad_token_id = 1,\n",
        "            position_embedding_type = \"absolute\",\n",
        "            tokenizer_class = \"BertTokenizer\",\n",
        "            transformers_version = \"4.20.0\",\n",
        "            type_vocab_size = 1,\n",
        "            use_cache = True,\n",
        "            vocab_size = 32000,\n",
        "            num_labels = 2\n",
        "        )  # 기존 모델의 config를 수정 후 저장\n",
        "\n",
        "        # 새 태스크를 위한 추가 출력층, 7개 출력을 받아서 2개로 분류\n",
        "        self.additional_classifier = nn.Linear(7, 2)\n",
        "\n",
        "    #AttributeError: 'ExtendedRobertaForSequenceClassification' object has no attribute 'can_generate' 해결용 함수. 이유 아직 모름\n",
        "    def can_generate(self):\n",
        "        return False  # 메서드로 변경\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels = None):\n",
        "        # 기존 모델을 통해 입력 데이터 처리\n",
        "        outputs = self.pretrained_model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        #Softmax 층 통과\n",
        "        #일단 없이 사용\n",
        "        # probs = torch.softmax(logits, dim = -1)\n",
        "\n",
        "        # 새로운 태스크의 예측을 위한 추가 출력층 사용\n",
        "        logits = self.additional_classifier(outputs.logits)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions\n",
        "        )\n",
        "\n",
        "# 확장된 모델 인스턴스 생성\n",
        "model = ExtendedRobertaForSequenceClassification(pretrained_model)"
      ],
      "metadata": {
        "id": "GivNmMtP-Npq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = pretrained_model\n",
        "#GPU 사용 코드\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#파이프라인을 사용하는 경우 GPU로 모델을 옮겨준 뒤 파이프라인에 모델을 넣어줘야 함\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "cCwLKdl6WWXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#전처리된 세 파일을 데이터프레임으로 로드\n",
        "df_qna = pd.read_excel('/content/drive/My Drive/community_qna_preprocessed_spaced.xlsx')\n",
        "df_all_contents = pd.read_excel('/content/drive/My Drive/community_all_contents_preprocessed_spaced.xlsx')\n",
        "df_all_comments = pd.read_excel('/content/drive/My Drive/community_all_comments_preprocessed_spaced.xlsx')"
      ],
      "metadata": {
        "id": "twzhDcrmkWra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "왜 빌더로그 게시글 요소들을 세 부분으로 나눠야 하는가?\n",
        "1. positive sample, negative sample 비율 1대1이 되도록 만들기\n",
        "2. 사용할 RobertaForSequenceClassification 아키텍처를 보면 (position_embeddings): Embedding(514, 768, padding_idx=1)이 포함되어 있다. negative sample에 해당하는 빌더로그 게시글만 길이가 길다면 이러한 정보까지 학습할 여지가 있다."
      ],
      "metadata": {
        "id": "P4ADRR6NZtQl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "왜 tokenizer()로 바로 시퀀스 길이를 구하지 않고 원본 리스트부터 시작해서 평균 길이를 구하는가\n",
        "->\n",
        "\n",
        "인자 중 max_length가 존재하기 때문에 어차피 길이를 초과하는 데이터는 truncation이 진행된다.\n",
        "\n",
        "그렇기 때문에 데이터 평균 길이를 비교하려면 원본 리스트를 사용해야 한다."
      ],
      "metadata": {
        "id": "27FqrJWYp-qQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터셋의 tokenized data의 길이를 구하는 함수\n",
        "def tokenized_sequence_length(text):\n",
        "  #kss 모듈로 글을 문장으로 분리\n",
        "  original_sentences = kss.split_sentences(text)\n",
        "  index_sentences = tokenizer(original_sentences)['input_ids']\n",
        "  index_sentences_filtered = [sentence[1:-1] for sentence in index_sentences] #cls_token과 sep_token 제외\n",
        "  total_length = sum(len(s) for s in index_sentences_filtered)\n",
        "  return index_sentences_filtered, total_length"
      ],
      "metadata": {
        "id": "Bl7lSvnjvz5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터셋에서 각 요소를 n등분하는 함수\n",
        "def divide_text_to_number(text, number):\n",
        "  #kss 모듈로 문장으로 분리\n",
        "  index_sentences, total_length = tokenized_sequence_length(text)\n",
        "  target_length = total_length / number\n",
        "\n",
        "  # 세 부분으로 분할\n",
        "  segments = []\n",
        "  current_segment = []\n",
        "  current_length = 0\n",
        "\n",
        "  #마지막 세그먼트 추가를 위해 사용하는 인덱스 값\n",
        "  sentence_checkpoint = 0\n",
        "\n",
        "  for index, sentence in enumerate(index_sentences):\n",
        "      if current_length + len(sentence) > target_length and current_segment:\n",
        "          segments.append(sum(current_segment, []))\n",
        "          current_segment = [sentence]\n",
        "          current_length = len(sentence)\n",
        "          if len(segments) == 2:  # 이미 두 세그먼트를 만들었다면 나머지는 모두 마지막 세그먼트에 추가\n",
        "              sentence_checkpoint = index #current_segment에 추가해둔 문장을 인덱싱\n",
        "              break\n",
        "      else:\n",
        "          current_segment.append(sentence)\n",
        "          current_length += len(sentence)\n",
        "\n",
        "  # 마지막 세그먼트 추가\n",
        "  segments.append(sum(current_segment + index_sentences[sentence_checkpoint + 1:], []))\n",
        "\n",
        "  return tokenizer.batch_decode(segments)\n"
      ],
      "metadata": {
        "id": "Eh1yITuCr_L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BP_judge_question용 데이터 제작\n",
        "\n",
        "#slug가 있다면 질문, 없다면 답변, 이 중 질문을 positive sample로 설정\n",
        "question_list = df_qna[pd.isna(df_qna['Slug']) == False]['내용'].dropna(axis = 0).to_list()\n",
        "label_question = [1] * len(question_list) #모델 설\n",
        "\n",
        "print(question_list)\n",
        "print(len(question_list))\n",
        "\n",
        "#질문 평균 길이 확인(원본 텍스트)\n",
        "print('원본 텍스트 평균 길이 :', sum(len(question) for question in question_list) / len(question_list))\n",
        "\n",
        "#질문 평균 길이 확인(토큰화 결과)\n",
        "print('토큰화된 텍스트 평균 길이 :', sum(tokenized_sequence_length(text)[1] for text in question_list) / len(question_list))"
      ],
      "metadata": {
        "id": "OjzfWDRxjr9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#답변을 negative sample로 설정\n",
        "answer_list = df_qna['답변'].dropna(axis = 0).to_list()\n",
        "label_answer = [0] * len(answer_list)\n",
        "\n",
        "print(answer_list)\n",
        "print(len(answer_list))\n",
        "\n",
        "#답변 평균 길이 확인(원본 텍스트)\n",
        "print('원본 텍스트 평균 길이 :', sum(len(answer) for answer in answer_list) / len(answer_list))\n",
        "\n",
        "#답변 평균 길이 확인(토큰화 결과)\n",
        "print('토큰화된 텍스트 평균 길이 :', sum(tokenized_sequence_length(text)[1] for text in answer_list) / len(answer_list))"
      ],
      "metadata": {
        "id": "8OppPmu0jxN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#빌더로그 글을 negative sample로 설정\n",
        "builder_log_list = df_all_contents[df_all_contents['커뮤니티 타입'] == '빌더 로그']['내용'].dropna(axis = 0).to_list()\n",
        "\n",
        "print(builder_log_list)\n",
        "print(len(builder_log_list))\n",
        "\n",
        "#빌더로그 글 평균 길이 확인(원본 텍스트)\n",
        "print('원본 텍스트 평균 길이 :', sum(len(builder_log) for builder_log in builder_log_list) / len(builder_log_list))\n",
        "\n",
        "#빌더로그 글 평균 길이 확인(토큰화 결과)\n",
        "print('토큰화된 텍스트 평균 길이 :', sum(tokenized_sequence_length(text)[1] for text in builder_log_list) / len(builder_log_list))\n",
        "\n",
        "#시퀀스 길이 균형을 위해 각 빌더로그 글을 3등분\n",
        "builder_log_list_divided = sum([divide_text_to_number(text, 3) for text in builder_log_list], [])\n",
        "\n",
        "#분할한 데이터의 개수에 맞춰서 레이블 설정\n",
        "label_builder_log = [0] * (len(builder_log_list_divided))\n",
        "\n",
        "#분리된 빌더로그 글 평균 길이 확인(토큰화 결과)\n",
        "print('분리된 데이터셋에서 토큰화된 텍스트 평균 길이 :', sum(tokenized_sequence_length(text)[1] for text in builder_log_list_divided) / len(builder_log_list_divided))"
      ],
      "metadata": {
        "id": "YZcOyOyKlwwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터프레임 생성\n",
        "text = question_list + answer_list + builder_log_list_divided\n",
        "label = label_question + label_answer + label_builder_log\n",
        "\n",
        "df_judge_question = pd.DataFrame([text, label], index = ['text', 'label'])\n",
        "df_judge_question = df_judge_question.T\n",
        "print(df_judge_question)"
      ],
      "metadata": {
        "id": "zdgwoXWzl5TA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#훈련(훈련+검증)/테스트 9:1 분리\n",
        "X, y = df_judge_question[\"text\"], df_judge_question[\"label\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, shuffle = True, stratify = y) #초기에는 정렬된 데이터셋으로 stratify로 레이블 비율을 유지하며 데이터셋을 섞은 후 분리\n",
        "\n",
        "#pandas dataframe으로 변환\n",
        "train_df = pd.DataFrame({\"text\": X_train, \"label\": y_train})\n",
        "test_df = pd.DataFrame({\"text\": X_test, \"label\": y_test})\n",
        "\n",
        "# Hugging Face의 Dataset 객체로 변환\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# 데이터셋 전처리 함수 및 처리\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=512)\n",
        "\n",
        "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "s1r5InapjtvY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "7d0ae957eafc473aa18f799141e7d06c",
            "a33768c23f0845e59bcd4e49b81ae9dd",
            "f149f4f343ba491cadd52d2acecbd362",
            "d9859841d45b4ad1b26ea2f30a2f9dc5",
            "e579426f57794038bf4f6e7015a7ae01",
            "37aa6ad2a934476081326b3245f39016",
            "24bca1ac22dd400983e87203f127a183",
            "fdad51b4c617401098b0ed8cffb26b1c",
            "987b8c4e6ca74bc5b2f2b20d2383a568",
            "6af43eae04aa469d8d158db0affa86ba",
            "ac3bc4c70ed4448398093c51a0f897fa"
          ]
        },
        "outputId": "b0236c8b-2ff4-4814-f570-e32055224564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/612 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7d0ae957eafc473aa18f799141e7d06c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#과적합 방지를 위한 EarlyStopping 클래스\n",
        "class EarlyStoppingCallback(TrainerCallback):\n",
        "    def __init__(self, patience=3):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def on_evaluate(self, args, state, control, **kwargs):\n",
        "        # 현재 검증 손실 가져오기\n",
        "        current_loss = kwargs['metrics']['eval_loss']\n",
        "\n",
        "        # 최고 성능 갱신 또는 카운터 증가\n",
        "        if self.best_score is None:\n",
        "            self.best_score = current_loss\n",
        "        elif current_loss > self.best_score:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "                control.should_training_stop = True\n",
        "        else:\n",
        "            self.best_score = current_loss\n",
        "            self.counter = 0"
      ],
      "metadata": {
        "id": "6hLwTTqHLQhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#분류 문제이며 레이블 간 데이터 수 비율을 고려하기 위해 StratifiedKFold 사용\n",
        "\n",
        "# StratifiedKFold 설정\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(tokenized_train_dataset, tokenized_train_dataset['label'])):\n",
        "\n",
        "    # 훈련 세트와 검증 세트 분리\n",
        "    train_dataset = tokenized_train_dataset.select(train_idx)\n",
        "    val_dataset = tokenized_train_dataset.select(val_idx)\n",
        "\n",
        "    # 훈련 설정\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        evaluation_strategy = \"steps\",\n",
        "        eval_steps = 500,\n",
        "        save_strategy = \"steps\",\n",
        "        save_steps = 500,\n",
        "        learning_rate = 2e-5,\n",
        "        num_train_epochs = 3, #고려사항\n",
        "        per_device_train_batch_size = 8,\n",
        "        per_device_eval_batch_size = 8,\n",
        "        warmup_steps = 100, #고려사항\n",
        "        weight_decay = 0.01, #고려사항\n",
        "        logging_dir = \"./logs\",\n",
        "\n",
        "        #\n",
        "    )\n",
        "\n",
        "    # 트레이너 초기화 및 훈련\n",
        "    #RobertaForSequenceClassification 모델은 기본적으로 분류 문제에 적합한 loss function을 내장하고 있습니다. num_labels=2로 설정함으로써, 이 모델은 이진 분류 문제를 위해 사전 구성되며, 내부적으로 Cross-Entropy Loss를 사용합니다.\n",
        "    trainer = Trainer(\n",
        "        model = model,\n",
        "        args = training_args,\n",
        "        train_dataset = train_dataset,\n",
        "        eval_dataset = val_dataset,\n",
        "        callbacks = [EarlyStoppingCallback(patience = 2)]\n",
        "    )\n",
        "\n",
        "    trainer.train()"
      ],
      "metadata": {
        "id": "wVgvrqOMjfGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#테스트 데이터셋 준비\n",
        "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "#테스트 데이터셋으로 모델 평가\n",
        "trainer.evaluate(eval_dataset = tokenized_test_dataset)"
      ],
      "metadata": {
        "id": "QANtqsM57Tam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#카카오톡 대화내용 불러오고 데이터프레임으로 받기\n",
        "file_path = '/content/drive/My Drive/talk_preprocess_result_short_spaced.xlsx'\n",
        "\n",
        "df = pd.read_excel(file_path)"
      ],
      "metadata": {
        "id": "5m1cJaKoYO59"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 처리 Pipeline 설정, top_k를 1로 설정시 최대로 측정된 label만 출력됨.\n",
        "#텍스트 처리해보니 시퀀스 길이가 539인 행 존재 -> truncation 진행, 최대 길이는 512\n",
        "text_classifier = TextClassificationPipeline(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    top_k = 1,\n",
        "    truncation = True,\n",
        "    batch_size = 128,\n",
        "    device = device\n",
        ")"
      ],
      "metadata": {
        "id": "m30_ktP_s7xC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #결측치 제거\n",
        "# df['text'] = df['text'].dropna(axis = 0)"
      ],
      "metadata": {
        "id": "nuaKEvDaMVbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #결측치 검증\n",
        "# print(type(text[:5]))\n",
        "# print(all(isinstance(item, str) for item in text))\n",
        "# for i, item in enumerate(text):\n",
        "#   if not isinstance(item, str):\n",
        "#     print(f\"Index {i}: {item} (Type: {type(item)})\")"
      ],
      "metadata": {
        "id": "6S6E11wljUEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#결측치 제거\n",
        "df = df.replace('None', np.nan).dropna(axis = 0)\n",
        "\n",
        "#모델로 prediction_labels 리스트 생성.\n",
        "text = df['text'].to_list()\n",
        "\n",
        "#tqdm으로 진행률 표시\n",
        "predictions= []\n",
        "for i in tqdm(range(0, len(text), 128), desc='Classifying'):\n",
        "    batch = text[i:i+128]\n",
        "    predictions.extend(text_classifier(batch))\n",
        "\n",
        "# predictions = text_classifier(text)\n",
        "prediction_labels = [prediction[0]['label'] for prediction in predictions]\n",
        "\n",
        "#Dataframe에 새 열로 추가.\n",
        "df['label'] = prediction_labels"
      ],
      "metadata": {
        "id": "Blp8I4xPG9hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#.csv 파일로 google drive에 저장\n",
        "save_path = '/content/drive/My Drive/judge_question_result_short.xlsx'\n",
        "\n",
        "df.to_excel(save_path)"
      ],
      "metadata": {
        "id": "oCXLMoGZvOcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L-REZQpC-86y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}