{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kimhansav/everynocode_search_engine/blob/main/BP_judge_answer_KCBERT_nsp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wabey716zPNq"
      },
      "outputs": [],
      "source": [
        "#한국 버블 커뮤니티 오픈톡방 대화의 질문에 대한 답변을 선별하는 코드(BERT Next Sentence Prediction)(로컬)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6eFub4T38V1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89778c25-65cd-4408-cbae-3fc053b6d29f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting soynlp\n",
            "  Downloading soynlp-0.0.493-py3-none-any.whl (416 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/416.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.8/416.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.8/416.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.10/dist-packages (from soynlp) (1.25.2)\n",
            "Requirement already satisfied: psutil>=5.0.1 in /usr/local/lib/python3.10/dist-packages (from soynlp) (5.9.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from soynlp) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from soynlp) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->soynlp) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->soynlp) (3.5.0)\n",
            "Installing collected packages: soynlp\n",
            "Successfully installed soynlp-0.0.493\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed datasets-2.19.1 dill-0.3.8 huggingface-hub-0.23.0 multiprocess-0.70.16 xxhash-3.4.1\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.30.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n",
            "Successfully installed accelerate-0.30.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install soynlp\n",
        "!pip install datasets\n",
        "!pip install accelerate -U\n",
        "import accelerate\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "import urllib.request\n",
        "from datasets import Dataset, load_dataset, ClassLabel\n",
        "from soynlp.word import WordExtractor\n",
        "from soynlp.tokenizer import LTokenizer, MaxScoreTokenizer\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from transformers import AutoTokenizer, shape_list, TFBertModel, RobertaTokenizerFast, RobertaForSequenceClassification, TextClassificationPipeline, pipeline, BertTokenizer, BertForNextSentencePrediction,  TrainingArguments, BertForMaskedLM, Trainer, TrainerCallback\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NSP로 훈련된 모델 로드\n",
        "finetuned_model_load_path = '/content/drive/My Drive/Finetuned_Model_judge_answer'\n",
        "\n",
        "finetuned_tokenizer = AutoTokenizer.from_pretrained(finetuned_model_load_path)\n",
        "finetuned_model = BertForNextSentencePrediction.from_pretrained(finetuned_model_load_path)"
      ],
      "metadata": {
        "id": "SI5nUJElRAgE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a75a91ad-1f11-4c13-da4d-e52b954b9874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 중요 : DataFrame의 Unnamed: 0을 사용해야 인덱스로 텍스트를 관리할 수 있다."
      ],
      "metadata": {
        "id": "65IhvclZ2JVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#url 삭제하지 않은 데이터셋 로드\n",
        "withurl_path = '/content/drive/My Drive/talk_preprocess_result_short_urlexist.xlsx'\n",
        "\n",
        "df_withurl = pd.read_excel(withurl_path)\n",
        "\n",
        "#열 이름 변경\n",
        "df_withurl.rename(columns={'Unnamed: 0': 'number'}, inplace=True)\n",
        "\n",
        "print(df_withurl)"
      ],
      "metadata": {
        "id": "w0SO3i8pkYeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcQirWwI0K1l"
      },
      "outputs": [],
      "source": [
        "#카카오톡 데이터 불러오기\n",
        "file_path = '/content/drive/My Drive/judge_question_result_short_KcBERT.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03vWAudThdM2"
      },
      "outputs": [],
      "source": [
        "#카카오톡 대화내용을 데이터프레임으로 받기\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "#Unnamed: 0 열을 사용하기 위해 열 이름 교체\n",
        "df.rename(columns={'Unnamed: 0': 'number'}, inplace=True)\n",
        "\n",
        "print(df)\n",
        "\n",
        "#질문으로 판별된 텍스트를 새 데이터프레임으로 생성\n",
        "df_question = df[df['label'] == 'question']\n",
        "\n",
        "print(df_question)\n",
        "\n",
        "#새 데이터프레임의 text, index, name, date를 질문 딕셔너리에 저장\n",
        "question_index_creator_date_dict = {text : {'index' : index, 'name' : name, 'date' : date} for (text, index, name, date) in zip(df_question['text'], df_question['number'], df_question['name'], df_question['date'])}\n",
        "print(question_index_creator_date_dict)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure using GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model = finetuned_model.to(device)\n",
        "tokenizer = finetuned_tokenizer"
      ],
      "metadata": {
        "id": "yyaMglos92s_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e41288f8-c280-42c4-9f1b-daf7c37b306e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfFU5ST9Nyau",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ffddd54-c3ca-405c-cbb0-d8d9deb8530b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Answer to Question: 100%|██████████| 7844/7844 [05:13<00:00, 25.04it/s]\n"
          ]
        }
      ],
      "source": [
        "#중복 질문 제거를 동시에 수행하기 위한 새로운 알고리즘\n",
        "\n",
        "#실험을 위한 하이퍼파라미터 설정\n",
        "#후보군으로 삼을 텍스트 개수 범위\n",
        "text_range = 20\n",
        "#답변 목록에 추가할지 기준이 되는 레이블값\n",
        "standard = 0.5\n",
        "\n",
        "#질문-답변 쌍 딕셔너리 생성. index로 질문 번호를 관리하기 때문에 value를 리스트로 작성하고, 리스트 안에는 {'question' : 질문내용, 'answer' : 답변내용, 'question_withurl' : url 포함한 질문내용, 'answer_withurl' : url 포함한 답변내용, 'questioner' : 작성자명, 'respondent' : 답변자 목록, 'date' : 작성날짜} 가 포함되도록 한다.\n",
        "qa_pair_dictionary = {info['index'] : {'question' : question, 'answer' : [], 'question_withurl' : df_withurl[df_withurl['number'] == info['index']]['text'].iloc[0], 'answer_withurl' : [], 'questioner' : info['name'], 'respondent' : [], 'date' : info['date']} for (question, info) in question_index_creator_date_dict.items()}\n",
        "\n",
        "#질문 속 질문인지 판별할 때 사용할 불리언\n",
        "in_question_texts = False\n",
        "\n",
        "all_texts = {index : {'text' : text, 'name' : name} for index, text, name in zip(df['number'], df['text'], df['name'])}\n",
        "\n",
        "for index, item in tqdm(all_texts.items(), desc = 'Processing Answer to Question'):\n",
        "\n",
        "  candidate_qa_list = [] #현재 텍스트의 소속을 판정할 (질문-답변 딕셔너리) 리스트\n",
        "  candidate_qa_index_list = []\n",
        "  in_question_texts = True if item['text'] in question_index_creator_date_dict else False #판별할 텍스트가 질문인지 검사 #df.iloc[index]['label'] == 'question'\n",
        "\n",
        "  start = 0 if index < 20 else index - text_range - 1 #인덱스가 20 미만일 경우 검사 범위 조정\n",
        "  for i in range(start, index): #현재 텍스트가 소속될 질문의 범위\n",
        "    candidate = qa_pair_dictionary.get(i, None) #qa_pair_dictionary에서 i 인덱스에 해당하는 질답 딕셔너리 가져오기\n",
        "    if candidate != None:\n",
        "      candidate_qa_list.append(candidate) #결과 리스트에서 최대 확률인 질문을 인덱싱하기 위해 인덱스를 포함한 딕셔너리를 append\n",
        "      candidate_qa_index_list.append(i)\n",
        "\n",
        "  if len(candidate_qa_list) == 0:\n",
        "    continue\n",
        "\n",
        "  #데이터를 튜플로 묶은 뒤 배치처리\n",
        "  batched_data = [(qa_dict['question'] + ' '.join(qa_dict['answer']), item['text']) for qa_dict in candidate_qa_list]\n",
        "  inputs = tokenizer(batched_data, padding = True, truncation = True, return_tensors = 'pt')\n",
        "  inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.softmax(logits, dim=-1)\n",
        "\n",
        "  outputs = [{'scores' : value, 'index' : index} for value, index in zip(probabilities.tolist(), candidate_qa_index_list)]\n",
        "\n",
        "  # 'scores'의 두 번째 값(연속적인 문장일 확률)에 따라 내림차순으로 정렬\n",
        "  sorted_output = sorted(outputs, key = lambda x : x['scores'][1], reverse=True)\n",
        "\n",
        "  output = sorted_output[0] #텍스트가 소속될 질문\n",
        "\n",
        "  if output['scores'][1] > standard:\n",
        "    #모순이 발생하지 않기 위해서 질문 속 질문으로 판별된 경우 즉시 qa_pair_dictionary에서 해당 질문을 삭제해야 함.\n",
        "    if in_question_texts == True:\n",
        "        del qa_pair_dictionary[index]\n",
        "\n",
        "    qa_pair_dictionary[output['index']]['answer'].append(item['text'])\n",
        "    qa_pair_dictionary[output['index']]['answer_withurl'].append(df_withurl[df_withurl['number'] == index]['text'].iloc[0])\n",
        "    qa_pair_dictionary[output['index']]['respondent'].append(item['name'])\n",
        "\n",
        "  # print('\\n',qa_pair_dictionary)\n",
        "  # print('\\n',used_question_index)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(qa_pair_dictionary)"
      ],
      "metadata": {
        "id": "LpnLBnCy3qkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_result= [item for item in qa_pair_dictionary.values()]\n",
        "\n",
        "#결과 데이터를 데이터프레임으로 변환\n",
        "df_result = pd.DataFrame(data = data_result, columns = ['question', 'answer', 'question_withurl', 'answer_withurl', 'questioner', 'respondent', 'date'])\n",
        "print(df_result)"
      ],
      "metadata": {
        "id": "wTt7Iw8W36_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#answer가 비어있는 행은 제거\n",
        "df_result = df_result[df_result['answer'].apply(lambda x: x != [])]\n",
        "print(len(df_result))"
      ],
      "metadata": {
        "id": "_fO1sF6T39i_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "830924e1-e676-4a90-89d3-bd50636166a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_result.iloc[0]['answer'] == [])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbZlvZ-U9uad",
        "outputId": "48c92da8-a966-4920-d833-d8c97662f91c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_result)"
      ],
      "metadata": {
        "id": "GCw57SD91ZTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXT7YkGs1Ede"
      },
      "outputs": [],
      "source": [
        "#csv 파일로 google drive에 저장\n",
        "#long 파일의 경우 illegalcharactererror로 인해서 .csv 파일로 저장하기\n",
        "save_path = '/content/drive/My Drive/judge_answer_result_KcBERT_short_questiontrained.csv'\n",
        "\n",
        "df_result.to_csv(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VRBK6UB4AiHe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}